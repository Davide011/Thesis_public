{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "#from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with entities(keys) and a assigning them an index number as dic value\n",
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "# chose ration of dataset used for ID and OOD\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr    # if we chose to take more data then in the array just take all the array\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]\n",
    "\n",
    "# Splits an array into two parts test-train  \n",
    "def split(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    train, test = [], []\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    for i in tqdm(range(len(arr))):\n",
    "        if i in rand_inds:\n",
    "            train.append(arr[i])\n",
    "        else:\n",
    "            test.append(arr[i])\n",
    "    return [train, test]\n",
    "\n",
    "def form_items(c, t):\n",
    "    #The join method concatenates all elements in the list c into a single string. For example, if c = [\"<e_1>\", \"<r_1>\"], then input_text will be \"<e_1><r_1>\".\n",
    "    input_text = \"\".join(c)\n",
    "    target_text = input_text + \"\".join([t, \"</a>\"]) # in  alist before for making more readable\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(num_entities, num_relations, out_degree=20, split_train_inferred=False):\n",
    " \n",
    "    # create a list with all entities names\n",
    "    entities = [\"<e_{}>\".format(i) for i in range(num_entities)]\n",
    "    # create a dictionary with entities as keys and index as values\n",
    "    ind2entity, entity2ind = build_dicts(entities)\n",
    "\n",
    "    # create a list with all relations names\n",
    "    relations = [\"<r_{}>\".format(i) for i in range(num_relations)]\n",
    "    # create a dictionary with relations as keys and index as values\n",
    "    ind2relation, relation2ind = build_dicts(relations)\n",
    "\n",
    "    #create atomic facts = dictionary head entity -> list of (relation, tail entity) pairs\n",
    "    atomic_dict = dict()   # maps a head entity to a list of (r, t) pairs\n",
    "    atomic_facts = []\n",
    "    atomics = []\n",
    "\n",
    "    for i in tqdm(range(num_entities)): #  it creates a progress bar that updates as the loop progresses\n",
    "        # for each subject entity, randomly select some outgoing relations to some random object entity\n",
    "        num_rows = out_degree\n",
    "        # randomly select some relations (for each head entity), size=num_rows is the number of relations to be selected=20= out_degree for node\n",
    "        selected_rows = np.random.choice(num_relations, size=num_rows, replace=False).tolist()\n",
    "        for row_idx in selected_rows:\n",
    "            col_idx = np.random.randint(num_entities)  # pick some random tail entity for each selected (h,r)\n",
    "            h,r,t = ind2entity[i], ind2relation[row_idx], ind2entity[col_idx]\n",
    "            atomic_facts.append(form_items([h, r], t))\n",
    "            atomics.append((h,r,t))\n",
    "            if h not in atomic_dict:  # add the head entity to the dictionary if it's not already there\n",
    "                atomic_dict[h] = []\n",
    "            # add the (r, t) pair to the list of pairs for this head entity. In this way a key (head)is associated to all the relations and tail entities\n",
    "            atomic_dict[h].append((r, t))   \n",
    "            ############################################################################################################################################  HERE end the ATOMIC FACTS CREATION!!!  ###############\n",
    "\n",
    "    if not split_train_inferred:   # if we don't want to split the training set into ID and OOD as onlly ID is needed skip this part    NOOOOOOOOOOOO USE\n",
    "        inferred_facts = []\n",
    "        for ent in tqdm(entities):\n",
    "            for (r1, b) in atomic_dict[ent]:  # for each (r1, b) pair associated with the head entity\n",
    "                for (r2, t) in atomic_dict[b]:   # for each (r2, t) pair associated with the bridge entity\n",
    "                    inferred_facts.append(form_items([ent, r1, r2], t))  # add the inferred fact to the list of inferred facts\n",
    "        return entities, relations, atomic_facts, inferred_facts\n",
    "    ################################################################################################################ continue with the split of the training set in ID and OOD\n",
    "    # split ID/OOD\n",
    "    OOD_ratio = 0.05          #atomic=list of tuples = all the atomic facts; atomics.append((h,r,t))\n",
    "    OOD_facts, ID_facts = split(atomics, round(len(atomics)*OOD_ratio)) #split the atomic facts in ID and OOD\n",
    "    OOD_facts, ID_facts = set(OOD_facts), set(ID_facts)                 #convert the lists OOD and ID in sets\n",
    "\n",
    "\n",
    "\n",
    "    # create a list of atomic facts for ID and OOD \"iteams\" see form_items function\n",
    "    # FROM A LIST OF TUPLES TO A LIST OF DICTIONARIES!!\n",
    "\n",
    "#OOD_facts = [(\"<e_1>\", \"<r_1>\", \"<e_2>\"), (\"<e_3>\", \"<r_2>\", \"<e_4>\")]\n",
    "# transform in:\n",
    "#ood_atomic_facts = [\n",
    "#   {\n",
    "#        \"input_text\": \"<e_1><r_1>\",\n",
    "#        \"target_text\": \"<e_1><r_1><e_2></a>\"  --> in other words:\"(h,r,t)\" + \"</a>\"\n",
    "#    },{\"input \".. } ..]\n",
    "  #################################################################################### create/convert the atomic in iteams way\n",
    "    id_atomic_facts = [form_items([h, r], t) for (h,r,t) in ID_facts]\n",
    "    ood_atomic_facts = [form_items([h, r], t) for (h,r,t) in OOD_facts]\n",
    "\n",
    "\n",
    "    ########  HERE WE CREATE THE TRAIN AND TEST SETS FOR THE INFERENCE TASK ########\n",
    "    #lets see what we train on and what we test on!\n",
    "\n",
    "    train_inferred_facts, test_inferred_iid, test_inferred_ood = [], [], []\n",
    "    t3_HOP_train_inferred_facts ,t3_HOP_test_inferred_iid , t3_HOP_test_inferred_ood = [], [], []      # 3-hop inference  (MY)\n",
    "\n",
    "    for ent in tqdm(entities):\n",
    "        #for each entity we loop over all the stored out_edges (relations and tail entities)   basically each entity is stored like= h : [(r1, b), (r2, c), (r3, d)] \n",
    "        for (r1, b) in atomic_dict[ent]:\n",
    "            for (r2, t) in atomic_dict[b]:   #now thre same looping but starting from the tail entity of before\n",
    "                if (ent, r1, b) in OOD_facts or (b, r2, t) in OOD_facts:  # at least one of the two entities is in the OOD set\n",
    "\n",
    "                    \n",
    "                    #if the head entity and the tail entity are in the OOD set, then we add the fact to the test set(OOD)\n",
    "                    if (ent, r1, b) in OOD_facts and (b, r2, t) in OOD_facts:\n",
    "                        test_inferred_ood.append(form_items([ent, r1, r2], t)) #if both entities are in the OOD set, we add the fact to the OOD list\n",
    "\n",
    "                    #continue # it exits the current iteration and goes to the next one (if both are in OOD we haveadded to OOD list otherwise we don't care either) \n",
    "                \n",
    "                else: \n",
    "                    #NOW if none of the two entities are in the OOD set we add the fact to the train set (randomly with a 0.5 probability)\n",
    "                    if np.random.uniform() > 0.005:\n",
    "                        train_inferred_facts.append(form_items([ent, r1, r2], t))\n",
    "                    else:\n",
    "                        test_inferred_iid.append(form_items([ent, r1, r2], t))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ###### 3-hop implementation\n",
    "                for (r3, t_3) in atomic_dict[t]:  # for each (r3, t) pair associated with the tail entity\n",
    "                    if (ent, r1, b) in OOD_facts or (b, r2, t) in OOD_facts or (t, r3, t_3) in OOD_facts:     #\n",
    "                        #print(\" or check 1\\n\")\n",
    "                        if (ent, r1, b) in OOD_facts and (b, r2, t) in OOD_facts and (t, r3, t_3) in OOD_facts:   # check if all the entities are in the OOD set\n",
    "                            print(\" or check 2\\n\")\n",
    "                            t3_HOP_test_inferred_ood.append(form_items([ent, r1, r2, r3], t_3))\n",
    "                        continue     \n",
    "                        #if at least one of the entities is in the OOD set,(but not all) we skip the current iteration = discard the inferred fact \n",
    "\n",
    "                    #if none of the entities are in the OOD set, we add the fact to the train set (randomly with a 0.5 probability)\n",
    "                    if np.random.uniform() > 0.005:\n",
    "                        t3_HOP_train_inferred_facts.append(form_items([ent, r1, r2, r3], t_3))\n",
    "                        \n",
    "                    else:\n",
    "                        t3_HOP_test_inferred_iid.append(form_items([ent, r1, r2, r3], t_3))\n",
    "                               \n",
    "            \n",
    "\n",
    "    return entities, relations, id_atomic_facts, ood_atomic_facts, train_inferred_facts, test_inferred_iid, test_inferred_ood , t3_HOP_train_inferred_facts ,t3_HOP_test_inferred_iid , t3_HOP_test_inferred_ood\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:00<00:00, 8868.29it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4000/4000 [00:00<00:00, 863780.88it/s]\n",
      "  2%|▏         | 3/200 [00:00<00:06, 28.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 18/200 [00:00<00:04, 41.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 28/200 [00:00<00:04, 42.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 38/200 [00:00<00:03, 42.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 48/200 [00:01<00:03, 41.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 58/200 [00:01<00:03, 42.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 68/200 [00:01<00:03, 43.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 83/200 [00:01<00:02, 42.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▋     | 93/200 [00:02<00:02, 43.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 113/200 [00:02<00:01, 43.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 123/200 [00:02<00:01, 43.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 133/200 [00:03<00:01, 43.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 143/200 [00:03<00:01, 43.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 153/200 [00:03<00:01, 42.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 163/200 [00:03<00:00, 42.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▋ | 173/200 [00:04<00:00, 42.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 183/200 [00:04<00:00, 42.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▋| 193/200 [00:04<00:00, 43.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200/200 [00:04<00:00, 42.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n",
      " or check 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUM_ENTITY_IN = 200    # complete 2000\n",
    "NUM_RELATION =  20     #complete 200   # 20\n",
    "\n",
    "train_entities, train_relations, id_atomic_facts, ood_atomic_facts, train_inferred_facts, test_inferred_iid, test_inferred_facts ,      t_3_train_inferred_facts, t_3_test_inferred_iid, t_3_test_inferred_facts = build_dataset(NUM_ENTITY_IN, NUM_RELATION, split_train_inferred=True)\n",
    "#train_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 226\n"
     ]
    }
   ],
   "source": [
    "# vocab is a list of all the entities and relations in the dataset =list of strings\n",
    "vocab = []\n",
    "vocab = vocab + train_entities + train_relations\n",
    "# special tokens added to the vocabulary for the model to understand the input and output format\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 300       #3000 complete\n",
    "id_atomic_facts_ds = choose(id_atomic_facts, test_size)\n",
    "ood_atomic_facts_ds = choose(ood_atomic_facts, test_size)\n",
    "test_inferred_iid = choose(test_inferred_iid, test_size) #IID\n",
    "test_inferred_facts_ds = choose(test_inferred_facts, test_size) #OOD\n",
    "\n",
    "all_atomics = id_atomic_facts + ood_atomic_facts\n",
    "len(all_atomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO/{}\".format(dataset_name), exist_ok=True)\n",
    "    train_inferred_facts_ds = choose(train_inferred_facts, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(train_inferred_facts_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3-hop inference save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "vocab size: 226\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# vocab is a list of all the entities and relations in the dataset =list of strings\n",
    "vocab = []\n",
    "print(vocab)   # to test the vocab list is refreshed\n",
    "vocab = vocab + train_entities + train_relations\n",
    "# special tokens added to the vocabulary for the model to understand the input and output format\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 300       #3000 complete\n",
    "id_atomic_facts_ds = choose(id_atomic_facts, test_size)   # the atomic facts are the same for all the datasets\n",
    "ood_atomic_facts_ds = choose(ood_atomic_facts, test_size)  # the atomic facts are the same for all the datasets\n",
    "# here change\n",
    "\n",
    "t_3_test_inferred_iid = choose(t_3_test_inferred_iid, test_size) #IID\n",
    "t_3_test_inferred_facts_ds = choose(t_3_test_inferred_facts, test_size) #OOD\n",
    "\n",
    "all_atomics = id_atomic_facts + ood_atomic_facts\n",
    "len(all_atomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"3_HOP_composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO/{}\".format(dataset_name), exist_ok=True)\n",
    "    t_3_train_inferred_facts_ds = choose(t_3_train_inferred_facts, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    \n",
    "    probes_3 = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    ######### change here for 3 hope \n",
    "\n",
    "    for item in choose(t_3_train_inferred_facts_ds, test_size):\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in t_3_test_inferred_iid:\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in t_3_test_inferred_facts_ds:\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + t_3_train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(t_3_test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(t_3_test_inferred_iid, f)\n",
    "\n",
    "    #test.json = probes_3 \n",
    "    with open(\"data_MIO/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes_3, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_transformers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
