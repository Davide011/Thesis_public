{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "#from tqdm.auto import tqdm\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import os\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary with entities(keys) and a assigning them an index number as dic value\n",
    "def build_dicts(entities):\n",
    "    entity2ind = dict()\n",
    "    ind2entity = []\n",
    "    for i in range(len(entities)):\n",
    "        entity = entities[i]\n",
    "        if not (entity in ind2entity):\n",
    "            ind2entity.append(entity)\n",
    "            entity2ind[entity] = len(ind2entity) - 1\n",
    "    return ind2entity, entity2ind\n",
    "\n",
    "# chose ration of dataset used for ID and OOD\n",
    "def choose(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    if num >= len(arr):\n",
    "        return arr    # if we chose to take more data then in the array just take all the array\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    return [arr[i] for i in rand_inds]\n",
    "\n",
    "# Splits an array into two parts test-train  \n",
    "def split(arr, ratio_or_count):\n",
    "    if type(ratio_or_count) == float:\n",
    "        num = round(ratio_or_count*len(arr))\n",
    "    elif type(ratio_or_count) == int:\n",
    "        num = ratio_or_count\n",
    "    else:\n",
    "         assert False\n",
    "    train, test = [], []\n",
    "    rand_inds = np.random.choice(len(arr), num, replace=False).tolist()\n",
    "    for i in tqdm(range(len(arr))):\n",
    "        if i in rand_inds:\n",
    "            train.append(arr[i])\n",
    "        else:\n",
    "            test.append(arr[i])\n",
    "    return [train, test]\n",
    "\n",
    "def form_items(c, t):\n",
    "    #The join method concatenates all elements in the list c into a single string. For example, if c = [\"<e_1>\", \"<r_1>\"], then input_text will be \"<e_1><r_1>\".\n",
    "    input_text = \"\".join(c)\n",
    "    target_text = input_text + \"\".join([t, \"</a>\"]) # in  alist before for making more readable\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item\n",
    "\n",
    "def form_items_SPLIT(c, t):\n",
    "    #The join method concatenates all elements in the list c into a single string. For example, if c = [\"<e_1>\", \"<r_1>\"], then input_text will be \"<e_1><r_1>\".\n",
    "    input_text = \"\".join(c+ [\"<mask>\"])\n",
    "    target_text = \"\".join(c+ [t, \"</a>\"]) # in  alist before for making more readable\n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    return item\n",
    "\n",
    "\n",
    "\n",
    "def form_items_SPLIT_inferred_2(c, d, t, hop_1):\n",
    "    # c = [ent, r1], d = r2, t = tail , hop_1 = b\n",
    "    # Concatenate elements from the list `c` with `<mask>`, `d`, and `<mask>` correctly\n",
    "    input_text = \"\".join(c + [\"<mask>\", d, \"<mask>\"])  # Concatenate c with <mask>, d, <mask>\n",
    "    \n",
    "    # Concatenate elements for the target_text\n",
    "    target_text = \"\".join(c + [hop_1, \"</a>\", d, t, \"</a>\"])  # Concatenate c with hop_1, \"</a>\", d, t, \"</a>\"\n",
    "    \n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    \n",
    "    return item\n",
    "\n",
    "def form_items_SPLIT_inferred_3(c,hop_1 ,r_2,hop_2, r_3,  t  ):\n",
    "    # Concatenate elements from the list `c` with `<mask>`, `d`, and `<mask>` correctly\n",
    "    # c = [ent, r1], d = r2, e = r3, t = tail , hop_1 = b, hop_2 = c\n",
    "    input_text = \"\".join(c + [\"<mask>\", r_2, \"<mask>\", r_3, \"<mask>\"])  # Concatenate c with <mask>, d, <mask>, e, <mask>\n",
    "    \n",
    "    # Concatenate elements for the target_text\n",
    "    target_text = \"\".join(c + [hop_1, \"</a>\", r_2, hop_2, \"</a>\", r_3, t, \"</a>\"])  # Concatenate c with hop_1, \"</a>\", d, hop_2, \"</a>\", e, t, \"</a>\"\n",
    "    \n",
    "    item = {\n",
    "        \"input_text\": input_text,\n",
    "        \"target_text\": target_text\n",
    "    }\n",
    "    \n",
    "    return item\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#test_inferred_ood.append(form_items([ent, r1],hop_1= b , r2, t)) \n",
    "\n",
    "#t3_HOP_test_inferred_ood.append(form_items([ent, r1],hop_1=b, r2, hop_2=c,  r3, t_3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': '<e_189><r_7><mask><r_2><mask><r_3><mask>',\n",
       " 'target_text': '<e_189><r_7><e_1></a><r_2><e_2></a><r_3><e_3></a>'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form_items_SPLIT_inferred_3([\"<e_189>\",\"<r_7>\"] ,hop_1= \"<e_1>\", r_2= \"<r_2>\",hop_2= \"<e_2>\", r_3= \"<r_3>\",  t= \"<e_3>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_text': '<e_176><r_7><mask><r__1><mask>',\n",
       " 'target_text': '<e_176><r_7><e_1></a><r__1><e_22></a>'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "form_items_SPLIT_inferred_2([\"<e_176>\",\"<r_7>\"] , \"<r__1>\", \"<e_22>\",hop_1= \"<e_1>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(num_entities, num_relations, out_degree=20, split_train_inferred=False):\n",
    " \n",
    "    # create a list with all entities names\n",
    "    entities = [\"<e_{}>\".format(i) for i in range(num_entities)]\n",
    "    # create a dictionary with entities as keys and index as values\n",
    "    ind2entity, entity2ind = build_dicts(entities)\n",
    "\n",
    "    # create a list with all relations names\n",
    "    relations = [\"<r_{}>\".format(i) for i in range(num_relations)]\n",
    "    # create a dictionary with relations as keys and index as values\n",
    "    ind2relation, relation2ind = build_dicts(relations)\n",
    "\n",
    "    #create atomic facts = dictionary head entity -> list of (relation, tail entity) pairs\n",
    "    atomic_dict = dict()   # maps a head entity to a list of (r, t) pairs\n",
    "    atomic_facts = []\n",
    "    atomics = []\n",
    "\n",
    "    for i in tqdm(range(num_entities)): #  it creates a progress bar that updates as the loop progresses\n",
    "        # for each subject entity, randomly select some outgoing relations to some random object entity\n",
    "        num_rows = out_degree\n",
    "        # randomly select some relations (for each head entity), size=num_rows is the number of relations to be selected=20= out_degree for node\n",
    "        selected_rows = np.random.choice(num_relations, size=num_rows, replace=False).tolist()\n",
    "        for row_idx in selected_rows:\n",
    "            col_idx = np.random.randint(num_entities)  # pick some random tail entity for each selected (h,r)\n",
    "            h,r,t = ind2entity[i], ind2relation[row_idx], ind2entity[col_idx]\n",
    "            atomic_facts.append(form_items([h, r], t))\n",
    "            atomics.append((h,r,t))\n",
    "            if h not in atomic_dict:  # add the head entity to the dictionary if it's not already there\n",
    "                atomic_dict[h] = []\n",
    "            # add the (r, t) pair to the list of pairs for this head entity. In this way a key (head)is associated to all the relations and tail entities\n",
    "            atomic_dict[h].append((r, t))   \n",
    "            ############################################################################################################################################  HERE end the ATOMIC FACTS CREATION!!!  ###############\n",
    "\n",
    "    if not split_train_inferred:   # if we don't want to split the training set into ID and OOD as onlly ID is needed skip this part    NOOOOOOOOOOOO USE\n",
    "        inferred_facts = []\n",
    "        for ent in tqdm(entities):\n",
    "            for (r1, b) in atomic_dict[ent]:  # for each (r1, b) pair associated with the head entity\n",
    "                for (r2, t) in atomic_dict[b]:   # for each (r2, t) pair associated with the bridge entity\n",
    "                    inferred_facts.append(form_items([ent, r1, r2], t))  # add the inferred fact to the list of inferred facts\n",
    "        return entities, relations, atomic_facts, inferred_facts\n",
    "    ################################################################################################################ continue with the split of the training set in ID and OOD\n",
    "    # split ID/OOD\n",
    "    OOD_ratio = 0.05          #atomic=list of tuples = all the atomic facts; atomics.append((h,r,t))\n",
    "    OOD_facts, ID_facts = split(atomics, round(len(atomics)*OOD_ratio)) #split the atomic facts in ID and OOD\n",
    "    OOD_facts, ID_facts = set(OOD_facts), set(ID_facts)                 #convert the lists OOD and ID in sets\n",
    "\n",
    "\n",
    "\n",
    "    # create a list of atomic facts for ID and OOD \"iteams\" see form_items function\n",
    "    # FROM A LIST OF TUPLES TO A LIST OF DICTIONARIES!!\n",
    "\n",
    "#OOD_facts = [(\"<e_1>\", \"<r_1>\", \"<e_2>\"), (\"<e_3>\", \"<r_2>\", \"<e_4>\")]\n",
    "# transform in:\n",
    "#ood_atomic_facts = [\n",
    "#   {\n",
    "#        \"input_text\": \"<e_1><r_1>\",\n",
    "#        \"target_text\": \"<e_1><r_1><e_2></a>\"  --> in other words:\"(h,r,t)\" + \"</a>\"\n",
    "#    },{\"input \".. } ..]\n",
    "  #################################################################################### create/convert the atomic in iteams way\n",
    "    id_atomic_facts = [form_items([h, r], t) for (h,r,t) in ID_facts]\n",
    "    ood_atomic_facts = [form_items([h, r], t) for (h,r,t) in OOD_facts]\n",
    "\n",
    "    ### MIO SPLIT the hops \n",
    "    SPLIT_id_atomic_facts =   [form_items_SPLIT([h, r], t) for (h,r,t) in ID_facts]  # just add rhe mask to the  \n",
    "    SPLIT_ood_atomic_facts =  [form_items_SPLIT([h, r], t) for (h,r,t) in OOD_facts]\n",
    "\n",
    "\n",
    "    cc=0\n",
    "    ########  HERE WE CREATE THE TRAIN AND TEST SETS FOR THE INFERENCE TASK ########\n",
    "    #lets see what we train on and what we test on!\n",
    "\n",
    "    train_inferred_facts, test_inferred_iid, test_inferred_ood = [], [], []\n",
    "    t3_HOP_train_inferred_facts ,t3_HOP_test_inferred_iid , t3_HOP_test_inferred_ood = [], [], []      # 3-hop inference  (MY)\n",
    "\n",
    "    # SPLIT the hops  adding <mask> to the input_text\n",
    "    SPLIT_train_inferred_facts, SPLIT_test_inferred_iid, SPLIT_test_inferred_ood = [], [], []\n",
    "\n",
    "    SPLIT_t3_HOP_train_inferred_facts ,SPLIT_t3_HOP_test_inferred_iid , SPLIT_t3_HOP_test_inferred_ood = [], [], []      # 3-hop inference  split\n",
    "\n",
    "    #### mask the only change is that in the training we put the ood in the second hop but coming from mask,mask,r_2\n",
    "    M_train_inferred_fact, M_test_inferred_iid, M_train_inferred_ood = [], [], []\n",
    "\n",
    "    M_M_train_inferred_fact, M_test_inferred_iid, M_M_train_inferred_ood = [], [], []\n",
    "    ID_M_M_train_inferred_fact, M_test_inferred_iid, ID_M_M_train_inferred_ood = [], [], []   # with also some ID (mask, Id)\n",
    "    OOD_allM_M_train_inferred_fact =[] # is used for putting in the training all the OOD in the second hop (by passing the phi % that\n",
    "    only_mask_ID=[]\n",
    "    # is a great disadvantage for OOD is there are 200 OOD atomic vs 3800 ID in case of 200_20 Small. That means that the % to not get \n",
    "    #one OOD in the second hop is much greater)\n",
    "\n",
    "\n",
    "    for ent in tqdm(entities):\n",
    "        #for each entity we loop over all the stored out_edges (relations and tail entities)   basically each entity is stored like= h : [(r1, b), (r2, c), (r3, d)] \n",
    "        for (r1, b) in atomic_dict[ent]:\n",
    "            for (r2, t) in atomic_dict[b]:   #now thre same looping but starting from the tail entity of before\n",
    "                if (ent, r1, b) in OOD_facts or (b, r2, t) in OOD_facts:  # at least one of the two entities is in the OOD set\n",
    "\n",
    "                    \n",
    "                    #if the head entity and the tail entity are in the OOD set, then we add the fact to the test set(OOD)\n",
    "                    if (ent, r1, b) in OOD_facts and (b, r2, t) in OOD_facts:\n",
    "                        test_inferred_ood.append(form_items([ent, r1, r2], t)) #if both entities are in the OOD set, we add the fact to the OOD list\n",
    "\n",
    "                        SPLIT_test_inferred_ood.append(form_items_SPLIT_inferred_2([ent, r1] , r2, t,hop_1= b))\n",
    "                        \n",
    "                        M_train_inferred_ood.append(form_items([\"<mask>\", \"<mask>\", r2], t))  # \n",
    "                        M_M_train_inferred_ood.append(form_items([\"<mask>\", b, r2], t))\n",
    "                    \n",
    "\n",
    "                        #form_items_SPLIT_inferred_2([\"<e_176>\",\"<r_7>\"] , \"<r__1>\", \"<e_22>\",hop_1= \"<e_1>\")\n",
    "                    #continue # it exits the current iteration and goes to the next one (if both are in OOD we haveadded to OOD list otherwise we don't care either) \n",
    "                \n",
    "                else: \n",
    "                    #NOW if none of the two entities are in the OOD set we add the fact to the train set (randomly with a 99.95% probability)\n",
    "                    if np.random.uniform() > 0.005:\n",
    "             \n",
    "                        train_inferred_facts.append(form_items([ent, r1, r2], t))\n",
    "                        ID_M_M_train_inferred_fact.append(form_items([\"<mask>\", b, r2], t))  # ad some id with mask, id as OOD (last test)\n",
    "                        only_mask_ID.append(form_items([\"<mask>\", b, r2], t))\n",
    "                        SPLIT_train_inferred_facts.append(form_items_SPLIT_inferred_2([ent, r1] , r2, t,hop_1= b))\n",
    "                    else:\n",
    "                        test_inferred_iid.append(form_items([ent, r1, r2], t))\n",
    "                        SPLIT_test_inferred_iid.append(form_items_SPLIT_inferred_2([ent, r1] , r2, t,hop_1= b))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                ###### 3-hop implementation\n",
    "                for (r3, t_3) in atomic_dict[t]:  # for each (r3, t) pair associated with the tail entity\n",
    "                    if (ent, r1, b) in OOD_facts or (b, r2, t) in OOD_facts or (t, r3, t_3) in OOD_facts:     #\n",
    "                        #print(\" or check 1\\n\")\n",
    "                        if (ent, r1, b) in OOD_facts and (b, r2, t) in OOD_facts and (t, r3, t_3) in OOD_facts:   # check if all the entities are in the OOD set\n",
    "                            #print(\" or check 2\\n\")\n",
    "                            t3_HOP_test_inferred_ood.append(form_items([ent, r1, r2, r3], t_3))\n",
    "                            SPLIT_t3_HOP_test_inferred_ood.append(form_items_SPLIT_inferred_3([ent, r1] ,hop_1= b, r_2= r2,hop_2= t, r_3= r3,  t= t_3))\n",
    "                        continue     \n",
    "                        #if at least one of the entities is in the OOD set,(but not all) we skip the current iteration = discard the inferred fact \n",
    "\n",
    "                    #if none of the entities are in the OOD set, we add the fact to the train set (randomly with a 0.5 probability)\n",
    "                    if np.random.uniform() > 0.005:\n",
    "                        t3_HOP_train_inferred_facts.append(form_items([ent, r1, r2, r3], t_3))\n",
    "                        SPLIT_t3_HOP_train_inferred_facts.append(form_items_SPLIT_inferred_3([ent, r1] ,hop_1= b, r_2= r2,hop_2= t, r_3= r3,  t= t_3))\n",
    "                        \n",
    "                    else:\n",
    "                        t3_HOP_test_inferred_iid.append(form_items([ent, r1, r2, r3], t_3))\n",
    "                        SPLIT_t3_HOP_test_inferred_iid.append(form_items_SPLIT_inferred_3([ent, r1] ,hop_1= b, r_2= r2,hop_2= t, r_3= r3,  t= t_3))\n",
    "                               \n",
    "            \n",
    "\n",
    "    ####\n",
    "    # Iterate through M_test_inferred_ood  (mask,mask)\n",
    "    M_train_inferred_fact.extend(copy.deepcopy(train_inferred_facts))\n",
    "    #M_train_inferred_fact.append(train_inferred_facts) #\n",
    "    # we add to the training the mas,mask inferred ood\n",
    "    for entry in M_train_inferred_ood:\n",
    "        #if np.random.uniform() > 0.005:\n",
    "        M_train_inferred_fact.append(entry)\n",
    "    # (mask b)\n",
    "    M_M_train_inferred_fact.extend(copy.deepcopy(train_inferred_facts))\n",
    "    # we add to the training the mas,mask inferred ood ( so normal train + mask ood)\n",
    "    for entry in M_M_train_inferred_ood:\n",
    "        #if np.random.uniform() > 0.005:\n",
    "        M_M_train_inferred_fact.append(entry)\n",
    "    #(mak, B)  with also ID\n",
    "    ID_M_M_train_inferred_fact.extend(copy.deepcopy(train_inferred_facts))\n",
    "    # we add to the training the mas,mask inferred ood and id\n",
    "    for entry in M_M_train_inferred_ood:\n",
    "        #if np.random.uniform() > 0.005:\n",
    "        ID_M_M_train_inferred_fact.append(entry)\n",
    "\n",
    "    print(\"len mask B\", len(M_M_train_inferred_fact))\n",
    "    print(\"len IDD  mask B\", len(ID_M_M_train_inferred_fact))\n",
    "    print(\"only ood\",len(M_M_train_inferred_ood))\n",
    "    print(\"train inf\",len( train_inferred_facts))\n",
    "\n",
    "\n",
    "    return (entities, relations, id_atomic_facts, ood_atomic_facts, train_inferred_facts, test_inferred_iid, test_inferred_ood , t3_HOP_train_inferred_facts ,t3_HOP_test_inferred_iid , t3_HOP_test_inferred_ood ,\n",
    "            \n",
    "             SPLIT_id_atomic_facts , SPLIT_ood_atomic_facts, SPLIT_train_inferred_facts, SPLIT_test_inferred_iid, SPLIT_test_inferred_ood , SPLIT_t3_HOP_train_inferred_facts ,SPLIT_t3_HOP_test_inferred_iid , SPLIT_t3_HOP_test_inferred_ood , \n",
    "             M_train_inferred_fact, M_M_train_inferred_fact , ID_M_M_train_inferred_fact  , M_M_train_inferred_ood , only_mask_ID)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:00<00:00, 8121.66it/s]\n",
      "100%|██████████| 40000/40000 [00:00<00:00, 107494.03it/s]\n",
      "100%|██████████| 2000/2000 [01:05<00:00, 30.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len mask B 720397\n",
      "len IDD  mask B 1438727\n",
      "only ood 2067\n",
      "train inf 718330\n"
     ]
    }
   ],
   "source": [
    "NUM_ENTITY_IN = 2000    # complete 2000\n",
    "NUM_RELATION =  200     #complete 200   # 20\n",
    " \n",
    "(train_entities, train_relations,\n",
    "id_atomic_facts,         ood_atomic_facts,          train_inferred_facts,   test_inferred_iid, test_inferred_facts ,      t_3_train_inferred_facts, t_3_test_inferred_iid, t_3_test_inferred_facts,\n",
    "SPLIT_id_atomic_facts , SPLIT_ood_atomic_facts, SPLIT_train_inferred_facts, SPLIT_test_inferred_iid, SPLIT_test_inferred_facts , SPLIT_t3_HOP_train_inferred_facts ,SPLIT_t3_HOP_test_inferred_iid , SPLIT_t3_HOP_test_inferred_facts,\n",
    "M_train_inferred_fact, M_M_train_inferred_fact ,ID_M_M_train_inferred_fact , OOD_mask_B_all ,only_mask_ID)  = build_dataset(NUM_ENTITY_IN, NUM_RELATION, split_train_inferred=True)\n",
    "\n",
    "#train_entities, train_relations are the same in all the cases\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aa 478800\n",
      "The tag <mask> appears 1361 times in the input text.\n"
     ]
    }
   ],
   "source": [
    "test_unione=train_inferred_facts + OOD_mask_B_all\n",
    "len(test_unione)\n",
    "for phi in [12.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    train_inferred_facts_ds_test_2 = choose(test_unione, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "    print(\"aa\",len(train_inferred_facts_ds_test_2))\n",
    "data_2=train_inferred_facts_ds_test_2\n",
    "mask_count_2= sum(item[\"input_text\"].count(\"<mask>\") for item in data_2)\n",
    "\n",
    "print(f\"The tag <mask> appears {mask_count_2} times in the input text.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478800\n",
      "The tag <mask> appears 1400 times in the input text.\n"
     ]
    }
   ],
   "source": [
    "for phi in [12.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "\n",
    "    train_inferred_facts_ds_test_1 = choose(M_M_train_inferred_fact, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "    print(len(train_inferred_facts_ds_test_1))\n",
    "data_1=train_inferred_facts_ds_test_1\n",
    "mask_count_1= sum(item[\"input_text\"].count(\"<mask>\") for item in data_1)\n",
    "\n",
    "print(f\"The tag <mask> appears {mask_count_1} times in the input text.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "478800\n",
      "The tag <mask> appears 239586 times in the input text.\n"
     ]
    }
   ],
   "source": [
    "for phi in [12.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "\n",
    "    train_inferred_facts_ds_test = choose(ID_M_M_train_inferred_fact, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "    print(len(train_inferred_facts_ds_test))\n",
    "\n",
    "data=train_inferred_facts_ds_test\n",
    "mask_count = sum(item[\"input_text\"].count(\"<mask>\") for item in data)\n",
    "\n",
    "print(f\"The tag <mask> appears {mask_count} times in the input text.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if any input text contains <mask> and print it\n",
    "for fact in train_inferred_facts:\n",
    "    if '<mask>' in fact['input_text']:\n",
    "        print(fact['input_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Explanation of build dataset output and renaming for dataset creation:\n",
    " > train_entities = list of all entities [e_0, e_1, e_2, ...]\n",
    "\n",
    ">train_relation=  list of all relations [r_0, r_1, r_2, ...]\n",
    "\n",
    ">id_atomic_facts/ood_atomic_facts=   list of atomic facts =list of dictionaries with 2keys\n",
    "\n",
    "    input_text(h,r) and target_text(h,r,t)  for each fact \n",
    "    \n",
    ">train_inferred_facts (= training inferred only on ID) =  ...\n",
    "\n",
    ">test_inferred_iid/test_inferred_facts(=test_inferred_ood)  =  ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 2206\n"
     ]
    }
   ],
   "source": [
    "# vocab is a list of all the entities and relations in the dataset =list of strings\n",
    "vocab = []\n",
    "vocab = vocab + train_entities + train_relations\n",
    "# special tokens added to the vocabulary for the model to understand the input and output format\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_size = 3000       #3000 complete\n",
    "id_atomic_facts_ds = choose(id_atomic_facts, test_size)\n",
    "ood_atomic_facts_ds = choose(ood_atomic_facts, test_size)\n",
    "test_inferred_iid_ds = choose(test_inferred_iid, test_size) #IID\n",
    "test_inferred_facts_ds = choose(test_inferred_facts, test_size) #OOD\n",
    "\n",
    "all_atomics = id_atomic_facts + ood_atomic_facts\n",
    "len(all_atomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    train_inferred_facts_ds = choose(train_inferred_facts, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(train_inferred_facts_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_FINAL/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_FINAL/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_FINAL/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO_FINAL/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_FINAL/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to add the info from the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask B augemnted with some OOD\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"mask_B_composition_OOD_some.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    M_M_train_inferred_facts_ds = choose(M_M_train_inferred_fact, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "    # augment the mask OOD in train (make sure they are not discared from the phi. However still this OOD_b_all is not really ALL!!!)\n",
    "    for ood_ in OOD_mask_B_all:\n",
    "        M_M_train_inferred_facts_ds.append(ood_)\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(M_M_train_inferred_facts_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_FINAL/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + M_M_train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_FINAL/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_FINAL/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO_FINAL/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_FINAL/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### her we make as before only OOD but we want to make the mask OOD =24 K  = half of the overall training inferred (otherwise only with aslo mask ID we have a right proportion between the normal iferred and the mask_B )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68400\n"
     ]
    }
   ],
   "source": [
    "M_M_train_inferred_facts_ds = choose(M_M_train_inferred_fact, round(phi * len(id_atomic_facts)/2)) #downsampling the train_inferred_facts (ID)\n",
    "print(len(M_M_train_inferred_facts_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000\n",
      "23000\n"
     ]
    }
   ],
   "source": [
    "# all OOD in 2-hop\n",
    "OOD_2_HOP_all=[]\n",
    "\n",
    "for fact in ood_atomic_facts:\n",
    "    target_text = fact['target_text']\n",
    "    parts = target_text.split('>')\n",
    "    \n",
    "    # Extract the first, second, and third parts\n",
    "    b = parts[0] + '>'\n",
    "    r2 = parts[1] + '>'\n",
    "    t = parts[2] + '>'\n",
    "    \n",
    "    # Print the extracted parts\n",
    "    #print(f\"b: {b}, r: {r}, t: {t}\")\n",
    "    OOD_2_HOP_all.append(form_items([\"<mask>\", b, r2], t))\n",
    "print(len(OOD_2_HOP_all))\n",
    "\n",
    "\n",
    "# Now we extend the list to 23 k\n",
    "\n",
    "# Calculate how many full repetitions are needed\n",
    "repetitions = 23000 // len(OOD_2_HOP_all)\n",
    "\n",
    "# Extend the list by repeating it and slicing to exactly 23,000 elements\n",
    "OOD_2_HOP_all_23K_extended = (OOD_2_HOP_all * (repetitions + 1))[:23000]\n",
    "print(len(OOD_2_HOP_all_23K_extended))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len FIRST of the total training inferred  342000\n",
      "len of the total training inferred + mask_B_OOD 365000 \n",
      "\n",
      "len FIRST of the total training inferred  239400\n",
      "len of the total training inferred + mask_B_OOD 262400 \n",
      "\n",
      "len FIRST of the total training inferred  171000\n",
      "len of the total training inferred + mask_B_OOD 194000 \n",
      "\n",
      "len FIRST of the total training inferred  136800\n",
      "len of the total training inferred + mask_B_OOD 159800 \n",
      "\n",
      "len FIRST of the total training inferred  102600\n",
      "len of the total training inferred + mask_B_OOD 125600 \n",
      "\n",
      "len FIRST of the total training inferred  68400\n",
      "len of the total training inferred + mask_B_OOD 91400 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mask B augemnted with ALL mask_B_OOD  until reach 23 K on a dataset of inferred+mask_B of  around 50K  ()\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"23K_mask_B_composition_OOD_some.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    M_M_train_inferred_facts_ds = choose(M_M_train_inferred_fact, round(phi * len(id_atomic_facts)/2)) #downsampling the train_inferred_facts  (we divede with 2  so we  have arounf 23k and we will add other 23 k from maks_b_ OOD\n",
    "    # augment the mask OOD in train (make sure they are not discared from the phi. However still this OOD_b_all is not really ALL!!!)\n",
    "    print(\"len FIRST of the total training inferred \",len( M_M_train_inferred_facts_ds))\n",
    "    for ood_ in OOD_2_HOP_all_23K_extended:\n",
    "        M_M_train_inferred_facts_ds.append(ood_)\n",
    "    print(\"len of the total training inferred + mask_B_OOD\",len( M_M_train_inferred_facts_ds),\"\\n\")\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(M_M_train_inferred_facts_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_FINAL/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + M_M_train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_FINAL/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_FINAL/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO_FINAL/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_FINAL/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask B normal not augmented\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"mask_B_composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    M_M_train_inferred_facts_ds_not_augmented = choose(M_M_train_inferred_fact, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(M_M_train_inferred_facts_ds_not_augmented, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_FINAL/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + M_M_train_inferred_facts_ds_not_augmented, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_FINAL/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_FINAL/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO_FINAL/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_FINAL/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case mask, B with also some  ID (mask id) to help the model to connect the generalization information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len ID normal 684000\n",
      "len ID normal 478800\n",
      "len ID normal 342000\n",
      "len ID normal 273600\n",
      "len ID normal 205200\n",
      "len ID normal 136800\n"
     ]
    }
   ],
   "source": [
    "# ID\n",
    "\n",
    "# downsampling train_inferred\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"ID_mask_B_composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    ID_M_M_train_inferred_facts_ds = choose(ID_M_M_train_inferred_fact, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "    print(\"len ID normal\", len(ID_M_M_train_inferred_facts_ds))\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(ID_M_M_train_inferred_facts_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_FINAL/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + ID_M_M_train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_FINAL/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_FINAL/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO_FINAL/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_FINAL/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "136800"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"ID_mask_B_composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    ID_M_M_train_inferred_facts_ds = choose(ID_M_M_train_inferred_fact, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "len(ID_M_M_train_inferred_facts_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the case wehere all OOD (mask,B_ood) are placed in the training. This is done bc the ID atomic are 3800 and OOD atomic only 200. This lead that when we apply the phi ration the % that one OOD does not appear in the second hop (or appears only rarely) affects the generalization for OOD. Instead the IID appear in IID inferred with much more cases, that means that by downsamplig with phi, it affect each much less. TO notice that with (200 OOD each OOD appear at maximum 199 times as second hop, The ID 3799). -case for Small dataset 200wnriries 20 relations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all OOD in 2-hop\n",
    "OOD_2_HOP_all=[]\n",
    "\n",
    "for fact in ood_atomic_facts:\n",
    "    target_text = fact['target_text']\n",
    "    parts = target_text.split('>')\n",
    "    \n",
    "    # Extract the first, second, and third parts\n",
    "    b = parts[0] + '>'\n",
    "    r2 = parts[1] + '>'\n",
    "    t = parts[2] + '>'\n",
    "    \n",
    "    # Print the extracted parts\n",
    "    #print(f\"b: {b}, r: {r}, t: {t}\")\n",
    "    OOD_2_HOP_all.append(form_items([\"<mask>\", b, r2], t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ID WITH ALL OOD addition: 686000\n",
      " ID WITH ALL OOD addition: 480800\n",
      " ID WITH ALL OOD addition: 344000\n",
      " ID WITH ALL OOD addition: 275600\n",
      " ID WITH ALL OOD addition: 207200\n",
      " ID WITH ALL OOD addition: 138800\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ID con additiion of REALLY ALL OOD\n",
    "\n",
    "# downsampling train_inferred\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"ID_mask_B_composition_ALL_OOD.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    ID_M_M_train_inferred_facts_ds_ALL_OOD = choose(ID_M_M_train_inferred_fact, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    for ood_ in OOD_2_HOP_all:\n",
    "        ID_M_M_train_inferred_facts_ds_ALL_OOD.append(ood_)\n",
    "\n",
    "    print(\" ID WITH ALL OOD addition:\", len(ID_M_M_train_inferred_facts_ds_ALL_OOD))\n",
    "\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(ID_M_M_train_inferred_facts_ds_ALL_OOD, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_FINAL/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + ID_M_M_train_inferred_facts_ds_ALL_OOD, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_FINAL/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_FINAL/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO_FINAL/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_FINAL/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'input_text': '<mask><e_1176><r_47>',\n",
       "  'target_text': '<mask><e_1176><r_47><e_344></a>'},\n",
       " {'input_text': '<mask><e_735><r_79>',\n",
       "  'target_text': '<mask><e_735><r_79><e_1448></a>'},\n",
       " {'input_text': '<mask><e_468><r_111>',\n",
       "  'target_text': '<mask><e_468><r_111><e_1311></a>'},\n",
       " {'input_text': '<mask><e_245><r_160>',\n",
       "  'target_text': '<mask><e_245><r_160><e_586></a>'},\n",
       " {'input_text': '<mask><e_1404><r_178>',\n",
       "  'target_text': '<mask><e_1404><r_178><e_230></a>'},\n",
       " {'input_text': '<mask><e_1507><r_79>',\n",
       "  'target_text': '<mask><e_1507><r_79><e_103></a>'},\n",
       " {'input_text': '<mask><e_1864><r_110>',\n",
       "  'target_text': '<mask><e_1864><r_110><e_1871></a>'},\n",
       " {'input_text': '<mask><e_148><r_133>',\n",
       "  'target_text': '<mask><e_148><r_133><e_451></a>'},\n",
       " {'input_text': '<mask><e_1576><r_165>',\n",
       "  'target_text': '<mask><e_1576><r_165><e_259></a>'},\n",
       " {'input_text': '<mask><e_1276><r_77>',\n",
       "  'target_text': '<mask><e_1276><r_77><e_1117></a>'},\n",
       " {'input_text': '<mask><e_341><r_153>',\n",
       "  'target_text': '<mask><e_341><r_153><e_1228></a>'},\n",
       " {'input_text': '<mask><e_183><r_129>',\n",
       "  'target_text': '<mask><e_183><r_129><e_532></a>'},\n",
       " {'input_text': '<mask><e_1572><r_69>',\n",
       "  'target_text': '<mask><e_1572><r_69><e_344></a>'},\n",
       " {'input_text': '<mask><e_1823><r_176>',\n",
       "  'target_text': '<mask><e_1823><r_176><e_1488></a>'},\n",
       " {'input_text': '<mask><e_1945><r_133>',\n",
       "  'target_text': '<mask><e_1945><r_133><e_841></a>'},\n",
       " {'input_text': '<mask><e_1792><r_197>',\n",
       "  'target_text': '<mask><e_1792><r_197><e_1531></a>'},\n",
       " {'input_text': '<mask><e_81><r_34>',\n",
       "  'target_text': '<mask><e_81><r_34><e_1281></a>'},\n",
       " {'input_text': '<mask><e_1580><r_44>',\n",
       "  'target_text': '<mask><e_1580><r_44><e_969></a>'},\n",
       " {'input_text': '<mask><e_1049><r_173>',\n",
       "  'target_text': '<mask><e_1049><r_173><e_777></a>'},\n",
       " {'input_text': '<mask><e_964><r_113>',\n",
       "  'target_text': '<mask><e_964><r_113><e_1476></a>'},\n",
       " {'input_text': '<mask><e_218><r_25>',\n",
       "  'target_text': '<mask><e_218><r_25><e_660></a>'},\n",
       " {'input_text': '<mask><e_52><r_186>',\n",
       "  'target_text': '<mask><e_52><r_186><e_1974></a>'},\n",
       " {'input_text': '<mask><e_885><r_103>',\n",
       "  'target_text': '<mask><e_885><r_103><e_1050></a>'},\n",
       " {'input_text': '<mask><e_16><r_30>',\n",
       "  'target_text': '<mask><e_16><r_30><e_725></a>'},\n",
       " {'input_text': '<mask><e_906><r_106>',\n",
       "  'target_text': '<mask><e_906><r_106><e_494></a>'},\n",
       " {'input_text': '<mask><e_1810><r_19>',\n",
       "  'target_text': '<mask><e_1810><r_19><e_1597></a>'},\n",
       " {'input_text': '<mask><e_1338><r_79>',\n",
       "  'target_text': '<mask><e_1338><r_79><e_1474></a>'},\n",
       " {'input_text': '<mask><e_798><r_199>',\n",
       "  'target_text': '<mask><e_798><r_199><e_1131></a>'},\n",
       " {'input_text': '<mask><e_1879><r_144>',\n",
       "  'target_text': '<mask><e_1879><r_144><e_269></a>'},\n",
       " {'input_text': '<mask><e_1218><r_127>',\n",
       "  'target_text': '<mask><e_1218><r_127><e_298></a>'},\n",
       " {'input_text': '<mask><e_1030><r_104>',\n",
       "  'target_text': '<mask><e_1030><r_104><e_1032></a>'},\n",
       " {'input_text': '<mask><e_1829><r_74>',\n",
       "  'target_text': '<mask><e_1829><r_74><e_266></a>'},\n",
       " {'input_text': '<mask><e_1740><r_109>',\n",
       "  'target_text': '<mask><e_1740><r_109><e_723></a>'},\n",
       " {'input_text': '<mask><e_1921><r_17>',\n",
       "  'target_text': '<mask><e_1921><r_17><e_761></a>'},\n",
       " {'input_text': '<mask><e_243><r_61>',\n",
       "  'target_text': '<mask><e_243><r_61><e_1261></a>'},\n",
       " {'input_text': '<mask><e_183><r_42>',\n",
       "  'target_text': '<mask><e_183><r_42><e_1461></a>'},\n",
       " {'input_text': '<mask><e_959><r_180>',\n",
       "  'target_text': '<mask><e_959><r_180><e_1256></a>'},\n",
       " {'input_text': '<mask><e_1618><r_176>',\n",
       "  'target_text': '<mask><e_1618><r_176><e_201></a>'},\n",
       " {'input_text': '<mask><e_1891><r_43>',\n",
       "  'target_text': '<mask><e_1891><r_43><e_875></a>'},\n",
       " {'input_text': '<mask><e_130><r_65>',\n",
       "  'target_text': '<mask><e_130><r_65><e_34></a>'},\n",
       " {'input_text': '<mask><e_229><r_41>',\n",
       "  'target_text': '<mask><e_229><r_41><e_715></a>'},\n",
       " {'input_text': '<mask><e_1852><r_58>',\n",
       "  'target_text': '<mask><e_1852><r_58><e_384></a>'},\n",
       " {'input_text': '<mask><e_1522><r_102>',\n",
       "  'target_text': '<mask><e_1522><r_102><e_613></a>'},\n",
       " {'input_text': '<mask><e_707><r_6>',\n",
       "  'target_text': '<mask><e_707><r_6><e_815></a>'},\n",
       " {'input_text': '<mask><e_250><r_63>',\n",
       "  'target_text': '<mask><e_250><r_63><e_982></a>'},\n",
       " {'input_text': '<mask><e_1054><r_154>',\n",
       "  'target_text': '<mask><e_1054><r_154><e_1008></a>'},\n",
       " {'input_text': '<mask><e_1932><r_118>',\n",
       "  'target_text': '<mask><e_1932><r_118><e_1365></a>'},\n",
       " {'input_text': '<mask><e_1808><r_56>',\n",
       "  'target_text': '<mask><e_1808><r_56><e_645></a>'},\n",
       " {'input_text': '<mask><e_1462><r_23>',\n",
       "  'target_text': '<mask><e_1462><r_23><e_1179></a>'},\n",
       " {'input_text': '<mask><e_1012><r_137>',\n",
       "  'target_text': '<mask><e_1012><r_137><e_1351></a>'},\n",
       " {'input_text': '<mask><e_1179><r_165>',\n",
       "  'target_text': '<mask><e_1179><r_165><e_1512></a>'},\n",
       " {'input_text': '<mask><e_752><r_111>',\n",
       "  'target_text': '<mask><e_752><r_111><e_1326></a>'},\n",
       " {'input_text': '<mask><e_1155><r_5>',\n",
       "  'target_text': '<mask><e_1155><r_5><e_1414></a>'},\n",
       " {'input_text': '<mask><e_283><r_195>',\n",
       "  'target_text': '<mask><e_283><r_195><e_1036></a>'},\n",
       " {'input_text': '<mask><e_501><r_137>',\n",
       "  'target_text': '<mask><e_501><r_137><e_1843></a>'},\n",
       " {'input_text': '<mask><e_1214><r_18>',\n",
       "  'target_text': '<mask><e_1214><r_18><e_518></a>'},\n",
       " {'input_text': '<mask><e_1995><r_169>',\n",
       "  'target_text': '<mask><e_1995><r_169><e_1919></a>'},\n",
       " {'input_text': '<mask><e_1461><r_20>',\n",
       "  'target_text': '<mask><e_1461><r_20><e_931></a>'},\n",
       " {'input_text': '<mask><e_1505><r_176>',\n",
       "  'target_text': '<mask><e_1505><r_176><e_1837></a>'},\n",
       " {'input_text': '<mask><e_1529><r_76>',\n",
       "  'target_text': '<mask><e_1529><r_76><e_854></a>'},\n",
       " {'input_text': '<mask><e_1724><r_178>',\n",
       "  'target_text': '<mask><e_1724><r_178><e_1375></a>'},\n",
       " {'input_text': '<mask><e_470><r_129>',\n",
       "  'target_text': '<mask><e_470><r_129><e_1618></a>'},\n",
       " {'input_text': '<mask><e_645><r_98>',\n",
       "  'target_text': '<mask><e_645><r_98><e_1818></a>'},\n",
       " {'input_text': '<mask><e_1778><r_72>',\n",
       "  'target_text': '<mask><e_1778><r_72><e_1250></a>'},\n",
       " {'input_text': '<mask><e_1296><r_166>',\n",
       "  'target_text': '<mask><e_1296><r_166><e_1706></a>'},\n",
       " {'input_text': '<mask><e_1191><r_80>',\n",
       "  'target_text': '<mask><e_1191><r_80><e_936></a>'},\n",
       " {'input_text': '<mask><e_793><r_186>',\n",
       "  'target_text': '<mask><e_793><r_186><e_403></a>'},\n",
       " {'input_text': '<mask><e_1410><r_154>',\n",
       "  'target_text': '<mask><e_1410><r_154><e_552></a>'},\n",
       " {'input_text': '<mask><e_574><r_102>',\n",
       "  'target_text': '<mask><e_574><r_102><e_30></a>'},\n",
       " {'input_text': '<mask><e_843><r_60>',\n",
       "  'target_text': '<mask><e_843><r_60><e_342></a>'},\n",
       " {'input_text': '<mask><e_661><r_37>',\n",
       "  'target_text': '<mask><e_661><r_37><e_207></a>'},\n",
       " {'input_text': '<mask><e_825><r_126>',\n",
       "  'target_text': '<mask><e_825><r_126><e_1018></a>'},\n",
       " {'input_text': '<mask><e_1004><r_148>',\n",
       "  'target_text': '<mask><e_1004><r_148><e_420></a>'},\n",
       " {'input_text': '<mask><e_428><r_43>',\n",
       "  'target_text': '<mask><e_428><r_43><e_1409></a>'},\n",
       " {'input_text': '<mask><e_1842><r_35>',\n",
       "  'target_text': '<mask><e_1842><r_35><e_1645></a>'},\n",
       " {'input_text': '<mask><e_1262><r_114>',\n",
       "  'target_text': '<mask><e_1262><r_114><e_853></a>'},\n",
       " {'input_text': '<mask><e_1616><r_188>',\n",
       "  'target_text': '<mask><e_1616><r_188><e_1147></a>'},\n",
       " {'input_text': '<mask><e_488><r_194>',\n",
       "  'target_text': '<mask><e_488><r_194><e_241></a>'},\n",
       " {'input_text': '<mask><e_58><r_105>',\n",
       "  'target_text': '<mask><e_58><r_105><e_533></a>'},\n",
       " {'input_text': '<mask><e_906><r_105>',\n",
       "  'target_text': '<mask><e_906><r_105><e_542></a>'},\n",
       " {'input_text': '<mask><e_1155><r_183>',\n",
       "  'target_text': '<mask><e_1155><r_183><e_385></a>'},\n",
       " {'input_text': '<mask><e_591><r_192>',\n",
       "  'target_text': '<mask><e_591><r_192><e_784></a>'},\n",
       " {'input_text': '<mask><e_1098><r_168>',\n",
       "  'target_text': '<mask><e_1098><r_168><e_48></a>'},\n",
       " {'input_text': '<mask><e_1544><r_78>',\n",
       "  'target_text': '<mask><e_1544><r_78><e_1285></a>'},\n",
       " {'input_text': '<mask><e_1882><r_32>',\n",
       "  'target_text': '<mask><e_1882><r_32><e_1913></a>'},\n",
       " {'input_text': '<mask><e_1720><r_112>',\n",
       "  'target_text': '<mask><e_1720><r_112><e_1235></a>'},\n",
       " {'input_text': '<mask><e_573><r_2>',\n",
       "  'target_text': '<mask><e_573><r_2><e_329></a>'},\n",
       " {'input_text': '<mask><e_1492><r_14>',\n",
       "  'target_text': '<mask><e_1492><r_14><e_974></a>'},\n",
       " {'input_text': '<mask><e_668><r_79>',\n",
       "  'target_text': '<mask><e_668><r_79><e_287></a>'},\n",
       " {'input_text': '<mask><e_1789><r_51>',\n",
       "  'target_text': '<mask><e_1789><r_51><e_804></a>'},\n",
       " {'input_text': '<mask><e_1046><r_76>',\n",
       "  'target_text': '<mask><e_1046><r_76><e_866></a>'},\n",
       " {'input_text': '<mask><e_1922><r_105>',\n",
       "  'target_text': '<mask><e_1922><r_105><e_1778></a>'},\n",
       " {'input_text': '<mask><e_1256><r_193>',\n",
       "  'target_text': '<mask><e_1256><r_193><e_35></a>'},\n",
       " {'input_text': '<mask><e_883><r_21>',\n",
       "  'target_text': '<mask><e_883><r_21><e_965></a>'},\n",
       " {'input_text': '<mask><e_472><r_132>',\n",
       "  'target_text': '<mask><e_472><r_132><e_1038></a>'},\n",
       " {'input_text': '<mask><e_1924><r_91>',\n",
       "  'target_text': '<mask><e_1924><r_91><e_449></a>'},\n",
       " {'input_text': '<mask><e_803><r_56>',\n",
       "  'target_text': '<mask><e_803><r_56><e_110></a>'},\n",
       " {'input_text': '<mask><e_164><r_128>',\n",
       "  'target_text': '<mask><e_164><r_128><e_455></a>'},\n",
       " {'input_text': '<mask><e_501><r_156>',\n",
       "  'target_text': '<mask><e_501><r_156><e_1449></a>'},\n",
       " {'input_text': '<mask><e_1165><r_98>',\n",
       "  'target_text': '<mask><e_1165><r_98><e_1546></a>'},\n",
       " {'input_text': '<mask><e_1892><r_111>',\n",
       "  'target_text': '<mask><e_1892><r_111><e_828></a>'},\n",
       " {'input_text': '<mask><e_1838><r_183>',\n",
       "  'target_text': '<mask><e_1838><r_183><e_428></a>'},\n",
       " {'input_text': '<mask><e_670><r_104>',\n",
       "  'target_text': '<mask><e_670><r_104><e_229></a>'},\n",
       " {'input_text': '<mask><e_109><r_55>',\n",
       "  'target_text': '<mask><e_109><r_55><e_501></a>'},\n",
       " {'input_text': '<mask><e_880><r_133>',\n",
       "  'target_text': '<mask><e_880><r_133><e_1937></a>'},\n",
       " {'input_text': '<mask><e_805><r_130>',\n",
       "  'target_text': '<mask><e_805><r_130><e_1123></a>'},\n",
       " {'input_text': '<mask><e_393><r_162>',\n",
       "  'target_text': '<mask><e_393><r_162><e_1214></a>'},\n",
       " {'input_text': '<mask><e_389><r_2>',\n",
       "  'target_text': '<mask><e_389><r_2><e_1965></a>'},\n",
       " {'input_text': '<mask><e_328><r_80>',\n",
       "  'target_text': '<mask><e_328><r_80><e_1210></a>'},\n",
       " {'input_text': '<mask><e_478><r_11>',\n",
       "  'target_text': '<mask><e_478><r_11><e_374></a>'},\n",
       " {'input_text': '<mask><e_1049><r_194>',\n",
       "  'target_text': '<mask><e_1049><r_194><e_381></a>'},\n",
       " {'input_text': '<mask><e_430><r_192>',\n",
       "  'target_text': '<mask><e_430><r_192><e_1776></a>'},\n",
       " {'input_text': '<mask><e_1513><r_26>',\n",
       "  'target_text': '<mask><e_1513><r_26><e_1542></a>'},\n",
       " {'input_text': '<mask><e_1432><r_170>',\n",
       "  'target_text': '<mask><e_1432><r_170><e_1497></a>'},\n",
       " {'input_text': '<mask><e_928><r_64>',\n",
       "  'target_text': '<mask><e_928><r_64><e_803></a>'},\n",
       " {'input_text': '<mask><e_882><r_139>',\n",
       "  'target_text': '<mask><e_882><r_139><e_675></a>'},\n",
       " {'input_text': '<mask><e_87><r_180>',\n",
       "  'target_text': '<mask><e_87><r_180><e_879></a>'},\n",
       " {'input_text': '<mask><e_719><r_7>',\n",
       "  'target_text': '<mask><e_719><r_7><e_1368></a>'},\n",
       " {'input_text': '<mask><e_1825><r_93>',\n",
       "  'target_text': '<mask><e_1825><r_93><e_796></a>'},\n",
       " {'input_text': '<mask><e_1708><r_180>',\n",
       "  'target_text': '<mask><e_1708><r_180><e_1603></a>'},\n",
       " {'input_text': '<mask><e_1371><r_61>',\n",
       "  'target_text': '<mask><e_1371><r_61><e_975></a>'},\n",
       " {'input_text': '<mask><e_219><r_123>',\n",
       "  'target_text': '<mask><e_219><r_123><e_429></a>'},\n",
       " {'input_text': '<mask><e_1860><r_54>',\n",
       "  'target_text': '<mask><e_1860><r_54><e_259></a>'},\n",
       " {'input_text': '<mask><e_1878><r_78>',\n",
       "  'target_text': '<mask><e_1878><r_78><e_1315></a>'},\n",
       " {'input_text': '<mask><e_246><r_183>',\n",
       "  'target_text': '<mask><e_246><r_183><e_714></a>'},\n",
       " {'input_text': '<mask><e_1513><r_190>',\n",
       "  'target_text': '<mask><e_1513><r_190><e_1497></a>'},\n",
       " {'input_text': '<mask><e_295><r_5>',\n",
       "  'target_text': '<mask><e_295><r_5><e_375></a>'},\n",
       " {'input_text': '<mask><e_840><r_85>',\n",
       "  'target_text': '<mask><e_840><r_85><e_655></a>'},\n",
       " {'input_text': '<mask><e_1038><r_90>',\n",
       "  'target_text': '<mask><e_1038><r_90><e_1076></a>'},\n",
       " {'input_text': '<mask><e_271><r_176>',\n",
       "  'target_text': '<mask><e_271><r_176><e_292></a>'},\n",
       " {'input_text': '<mask><e_1597><r_125>',\n",
       "  'target_text': '<mask><e_1597><r_125><e_1762></a>'},\n",
       " {'input_text': '<mask><e_510><r_162>',\n",
       "  'target_text': '<mask><e_510><r_162><e_1406></a>'},\n",
       " {'input_text': '<mask><e_1445><r_37>',\n",
       "  'target_text': '<mask><e_1445><r_37><e_4></a>'},\n",
       " {'input_text': '<mask><e_1806><r_34>',\n",
       "  'target_text': '<mask><e_1806><r_34><e_439></a>'},\n",
       " {'input_text': '<mask><e_8><r_127>',\n",
       "  'target_text': '<mask><e_8><r_127><e_1197></a>'},\n",
       " {'input_text': '<mask><e_314><r_61>',\n",
       "  'target_text': '<mask><e_314><r_61><e_258></a>'},\n",
       " {'input_text': '<mask><e_589><r_126>',\n",
       "  'target_text': '<mask><e_589><r_126><e_1444></a>'},\n",
       " {'input_text': '<mask><e_804><r_126>',\n",
       "  'target_text': '<mask><e_804><r_126><e_801></a>'},\n",
       " {'input_text': '<mask><e_604><r_133>',\n",
       "  'target_text': '<mask><e_604><r_133><e_247></a>'},\n",
       " {'input_text': '<mask><e_115><r_118>',\n",
       "  'target_text': '<mask><e_115><r_118><e_1464></a>'},\n",
       " {'input_text': '<mask><e_1><r_63>',\n",
       "  'target_text': '<mask><e_1><r_63><e_515></a>'},\n",
       " {'input_text': '<mask><e_1371><r_63>',\n",
       "  'target_text': '<mask><e_1371><r_63><e_1884></a>'},\n",
       " {'input_text': '<mask><e_1524><r_154>',\n",
       "  'target_text': '<mask><e_1524><r_154><e_633></a>'},\n",
       " {'input_text': '<mask><e_1412><r_183>',\n",
       "  'target_text': '<mask><e_1412><r_183><e_351></a>'},\n",
       " {'input_text': '<mask><e_1331><r_154>',\n",
       "  'target_text': '<mask><e_1331><r_154><e_1589></a>'},\n",
       " {'input_text': '<mask><e_1988><r_153>',\n",
       "  'target_text': '<mask><e_1988><r_153><e_357></a>'},\n",
       " {'input_text': '<mask><e_931><r_85>',\n",
       "  'target_text': '<mask><e_931><r_85><e_1842></a>'},\n",
       " {'input_text': '<mask><e_695><r_187>',\n",
       "  'target_text': '<mask><e_695><r_187><e_432></a>'},\n",
       " {'input_text': '<mask><e_1721><r_32>',\n",
       "  'target_text': '<mask><e_1721><r_32><e_497></a>'},\n",
       " {'input_text': '<mask><e_1734><r_67>',\n",
       "  'target_text': '<mask><e_1734><r_67><e_787></a>'},\n",
       " {'input_text': '<mask><e_1356><r_55>',\n",
       "  'target_text': '<mask><e_1356><r_55><e_967></a>'},\n",
       " {'input_text': '<mask><e_1878><r_106>',\n",
       "  'target_text': '<mask><e_1878><r_106><e_99></a>'},\n",
       " {'input_text': '<mask><e_113><r_133>',\n",
       "  'target_text': '<mask><e_113><r_133><e_1237></a>'},\n",
       " {'input_text': '<mask><e_1654><r_122>',\n",
       "  'target_text': '<mask><e_1654><r_122><e_729></a>'},\n",
       " {'input_text': '<mask><e_720><r_74>',\n",
       "  'target_text': '<mask><e_720><r_74><e_1014></a>'},\n",
       " {'input_text': '<mask><e_1227><r_180>',\n",
       "  'target_text': '<mask><e_1227><r_180><e_679></a>'},\n",
       " {'input_text': '<mask><e_1817><r_145>',\n",
       "  'target_text': '<mask><e_1817><r_145><e_1541></a>'},\n",
       " {'input_text': '<mask><e_629><r_70>',\n",
       "  'target_text': '<mask><e_629><r_70><e_32></a>'},\n",
       " {'input_text': '<mask><e_1153><r_132>',\n",
       "  'target_text': '<mask><e_1153><r_132><e_1675></a>'},\n",
       " {'input_text': '<mask><e_1486><r_48>',\n",
       "  'target_text': '<mask><e_1486><r_48><e_1153></a>'},\n",
       " {'input_text': '<mask><e_1537><r_151>',\n",
       "  'target_text': '<mask><e_1537><r_151><e_648></a>'},\n",
       " {'input_text': '<mask><e_384><r_163>',\n",
       "  'target_text': '<mask><e_384><r_163><e_1097></a>'},\n",
       " {'input_text': '<mask><e_587><r_119>',\n",
       "  'target_text': '<mask><e_587><r_119><e_1935></a>'},\n",
       " {'input_text': '<mask><e_1644><r_43>',\n",
       "  'target_text': '<mask><e_1644><r_43><e_578></a>'},\n",
       " {'input_text': '<mask><e_141><r_191>',\n",
       "  'target_text': '<mask><e_141><r_191><e_790></a>'},\n",
       " {'input_text': '<mask><e_890><r_188>',\n",
       "  'target_text': '<mask><e_890><r_188><e_1359></a>'},\n",
       " {'input_text': '<mask><e_626><r_4>',\n",
       "  'target_text': '<mask><e_626><r_4><e_782></a>'},\n",
       " {'input_text': '<mask><e_1819><r_67>',\n",
       "  'target_text': '<mask><e_1819><r_67><e_372></a>'},\n",
       " {'input_text': '<mask><e_870><r_66>',\n",
       "  'target_text': '<mask><e_870><r_66><e_806></a>'},\n",
       " {'input_text': '<mask><e_1093><r_115>',\n",
       "  'target_text': '<mask><e_1093><r_115><e_1497></a>'},\n",
       " {'input_text': '<mask><e_1265><r_38>',\n",
       "  'target_text': '<mask><e_1265><r_38><e_1147></a>'},\n",
       " {'input_text': '<mask><e_1539><r_126>',\n",
       "  'target_text': '<mask><e_1539><r_126><e_1325></a>'},\n",
       " {'input_text': '<mask><e_1258><r_194>',\n",
       "  'target_text': '<mask><e_1258><r_194><e_1073></a>'},\n",
       " {'input_text': '<mask><e_1558><r_23>',\n",
       "  'target_text': '<mask><e_1558><r_23><e_1864></a>'},\n",
       " {'input_text': '<mask><e_681><r_125>',\n",
       "  'target_text': '<mask><e_681><r_125><e_1844></a>'},\n",
       " {'input_text': '<mask><e_1240><r_198>',\n",
       "  'target_text': '<mask><e_1240><r_198><e_177></a>'},\n",
       " {'input_text': '<mask><e_604><r_174>',\n",
       "  'target_text': '<mask><e_604><r_174><e_484></a>'},\n",
       " {'input_text': '<mask><e_550><r_29>',\n",
       "  'target_text': '<mask><e_550><r_29><e_1479></a>'},\n",
       " {'input_text': '<mask><e_743><r_128>',\n",
       "  'target_text': '<mask><e_743><r_128><e_585></a>'},\n",
       " {'input_text': '<mask><e_1128><r_95>',\n",
       "  'target_text': '<mask><e_1128><r_95><e_492></a>'},\n",
       " {'input_text': '<mask><e_1996><r_130>',\n",
       "  'target_text': '<mask><e_1996><r_130><e_1548></a>'},\n",
       " {'input_text': '<mask><e_1841><r_128>',\n",
       "  'target_text': '<mask><e_1841><r_128><e_445></a>'},\n",
       " {'input_text': '<mask><e_578><r_29>',\n",
       "  'target_text': '<mask><e_578><r_29><e_1369></a>'},\n",
       " {'input_text': '<mask><e_584><r_61>',\n",
       "  'target_text': '<mask><e_584><r_61><e_1075></a>'},\n",
       " {'input_text': '<mask><e_1161><r_138>',\n",
       "  'target_text': '<mask><e_1161><r_138><e_666></a>'},\n",
       " {'input_text': '<mask><e_1058><r_5>',\n",
       "  'target_text': '<mask><e_1058><r_5><e_1340></a>'},\n",
       " {'input_text': '<mask><e_1735><r_191>',\n",
       "  'target_text': '<mask><e_1735><r_191><e_1103></a>'},\n",
       " {'input_text': '<mask><e_1906><r_114>',\n",
       "  'target_text': '<mask><e_1906><r_114><e_844></a>'},\n",
       " {'input_text': '<mask><e_237><r_61>',\n",
       "  'target_text': '<mask><e_237><r_61><e_1265></a>'},\n",
       " {'input_text': '<mask><e_1714><r_25>',\n",
       "  'target_text': '<mask><e_1714><r_25><e_642></a>'},\n",
       " {'input_text': '<mask><e_341><r_69>',\n",
       "  'target_text': '<mask><e_341><r_69><e_208></a>'},\n",
       " {'input_text': '<mask><e_1071><r_93>',\n",
       "  'target_text': '<mask><e_1071><r_93><e_1694></a>'},\n",
       " {'input_text': '<mask><e_1366><r_8>',\n",
       "  'target_text': '<mask><e_1366><r_8><e_497></a>'},\n",
       " {'input_text': '<mask><e_667><r_6>',\n",
       "  'target_text': '<mask><e_667><r_6><e_1168></a>'},\n",
       " {'input_text': '<mask><e_47><r_115>',\n",
       "  'target_text': '<mask><e_47><r_115><e_485></a>'},\n",
       " {'input_text': '<mask><e_1080><r_89>',\n",
       "  'target_text': '<mask><e_1080><r_89><e_388></a>'},\n",
       " {'input_text': '<mask><e_874><r_88>',\n",
       "  'target_text': '<mask><e_874><r_88><e_1552></a>'},\n",
       " {'input_text': '<mask><e_1026><r_170>',\n",
       "  'target_text': '<mask><e_1026><r_170><e_1245></a>'},\n",
       " {'input_text': '<mask><e_374><r_153>',\n",
       "  'target_text': '<mask><e_374><r_153><e_1695></a>'},\n",
       " {'input_text': '<mask><e_201><r_155>',\n",
       "  'target_text': '<mask><e_201><r_155><e_1129></a>'}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_inferred_chosen=choose(only_mask_ID,200)\n",
    "len(ID_inferred_chosen)\n",
    "ID_inferred_chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "304200"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_ID_M_M_train_inferred_facts_ds_ALL_OOD_test = choose(M_M_train_inferred_fact, round(8 * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "len(some_ID_M_M_train_inferred_facts_ds_ALL_OOD_test)\n",
    "\n",
    "some_ID_M_M_train_inferred_facts_ds_ALL_OOD_test.extend(ID_inferred_chosen)\n",
    "len(some_ID_M_M_train_inferred_facts_ds_ALL_OOD_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ALL OOD doubel and some ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " some ID WITH ALL OOD addition: 686400\n",
      " some ID WITH ALL OOD addition: 481200\n",
      " some ID WITH ALL OOD addition: 344400\n",
      " some ID WITH ALL OOD addition: 276000\n",
      " some ID WITH ALL OOD addition: 207600\n",
      " some ID WITH ALL OOD addition: 139200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ID con additiion of REALLY ALL OOD\n",
    "\n",
    "# downsampling train_inferred\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"some_ID_mask_B_composition_ALL_OOD.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    some_ID_M_M_train_inferred_facts_ds_ALL_OOD = choose(M_M_train_inferred_fact, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    for ood_ in OOD_2_HOP_all:\n",
    "        some_ID_M_M_train_inferred_facts_ds_ALL_OOD.append(ood_)\n",
    "    ID_inferred_chosen=choose(only_mask_ID,400)\n",
    "    some_ID_M_M_train_inferred_facts_ds_ALL_OOD.extend(ID_inferred_chosen)\n",
    "\n",
    "    print(\" some ID WITH ALL OOD addition:\", len(some_ID_M_M_train_inferred_facts_ds_ALL_OOD))\n",
    "\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(some_ID_M_M_train_inferred_facts_ds_ALL_OOD, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_FINAL/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics +some_ID_M_M_train_inferred_facts_ds_ALL_OOD, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_FINAL/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_FINAL/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO_FINAL/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_FINAL/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ID WITH ALL OOD addition: 136800\n"
     ]
    }
   ],
   "source": [
    "print(\" ID WITH ALL OOD addition:\", len(ID_M_M_train_inferred_facts_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43ma\u001b[49m\n\u001b[1;32m      3\u001b[0m b\n\u001b[1;32m      4\u001b[0m v\n",
      "\u001b[0;31mNameError\u001b[0m: name 'a' is not defined"
     ]
    }
   ],
   "source": [
    "a\n",
    "\n",
    "b\n",
    "v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Without double hops\n",
    "only mask,B_id\n",
    "mask,B_OOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOD_mask_B_all ,only_mask_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " NO 2-hop only mask_B: 69200\n",
      " NO 2-hop only mask_B: 48680\n",
      " NO 2-hop only mask_B: 35000\n",
      " NO 2-hop only mask_B: 28160\n",
      " NO 2-hop only mask_B: 21320\n",
      " NO 2-hop only mask_B: 14480\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "# ID con additiion of REALLY ALL OOD\n",
    "\n",
    "# downsampling train_inferred\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"no_2_hop_with_some.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_FINAL/{}\".format(dataset_name), exist_ok=True)\n",
    "    no_2_hop_ds = choose(only_mask_ID, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    for ood_ in OOD_2_HOP_all:\n",
    "        no_2_hop_ds.append(ood_)\n",
    "    for ood_ in OOD_2_HOP_all:\n",
    "        no_2_hop_ds.append(ood_)\n",
    "    ID_inferred_chosen_1=choose(train_inferred_facts,400)\n",
    "    no_2_hop_ds.extend(ID_inferred_chosen_1)\n",
    "   \n",
    "\n",
    "    print(\" NO 2-hop only mask_B:\", len(no_2_hop_ds))\n",
    "\n",
    "    probes = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(no_2_hop_ds, test_size):\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in test_inferred_iid:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in test_inferred_facts_ds:\n",
    "        probes.append(deepcopy(item))\n",
    "        probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_FINAL/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics +no_2_hop_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_FINAL/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_FINAL/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes (all the atomic facts and the inferred facts how I know are not used in training??)\n",
    "    with open(\"data_MIO_FINAL/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_FINAL/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stop here for just 2-hop normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SPLIT HOPS 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atomic facts are slightly differently written (mask) for the SPLIT and NO split case but are the same for 2 ir 3 hop\n",
    "\n",
    "test_size= 300\n",
    "SPLIT_id_atomic_facts_ds = choose(SPLIT_id_atomic_facts, test_size)\n",
    "SPLIT_ood_atomic_facts_ds = choose(SPLIT_ood_atomic_facts, test_size)\n",
    "SPLIT_test_inferred_iid_ds = choose(SPLIT_test_inferred_iid, test_size) #IID\n",
    "SPLIT_test_inferred_facts_ds = choose(SPLIT_test_inferred_facts, test_size) #OOD\n",
    "\n",
    "SPLIT_all_atomics = SPLIT_id_atomic_facts + SPLIT_ood_atomic_facts\n",
    "len(SPLIT_all_atomics)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  \n",
    "    dataset_name = \"SPLIT_composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_mask_NEW_correct/{}\".format(dataset_name), exist_ok=True)\n",
    "    SPLIT_train_inferred_facts_ds = choose(SPLIT_train_inferred_facts, round(phi * len(SPLIT_id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    SPLIT_probes = []\n",
    "    for item in SPLIT_id_atomic_facts_ds:\n",
    "        SPLIT_probes.append(deepcopy(item))\n",
    "        SPLIT_probes[-1][\"type\"] = \"id_atomic\"\n",
    "\n",
    "    for item in SPLIT_ood_atomic_facts_ds:\n",
    "        SPLIT_probes.append(deepcopy(item))\n",
    "        SPLIT_probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    for item in choose(SPLIT_train_inferred_facts_ds, test_size):\n",
    "        SPLIT_probes.append(deepcopy(item))\n",
    "        SPLIT_probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in SPLIT_test_inferred_iid:\n",
    "        SPLIT_probes.append(deepcopy(item))\n",
    "        SPLIT_probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in SPLIT_test_inferred_facts_ds:\n",
    "        SPLIT_probes.append(deepcopy(item))\n",
    "        SPLIT_probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(SPLIT_all_atomics + SPLIT_train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?)\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(SPLIT_test_inferred_facts_ds, f)\n",
    "    \n",
    "    # my validation with the test_inferred_iid\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(SPLIT_test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = \n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(SPLIT_probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-hop inference save\n",
    "\n",
    "the vocab is actually the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# vocab is a list of all the entities and relations in the dataset =list of strings\n",
    "vocab = []\n",
    "print(vocab)   # to test the vocab list is refreshed\n",
    "vocab = vocab + train_entities + train_relations\n",
    "# special tokens added to the vocabulary for the model to understand the input and output format\n",
    "vocab = vocab + [\"<mask>\", \"<sep>\", \"<a>\", \"</a>\", \"<q>\", \"</q>\"]\n",
    "assert len(vocab) == len(set(vocab))\n",
    "print(\"vocab size:\", len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 300       #3000 complete\n",
    "id_atomic_facts_ds = choose(id_atomic_facts, test_size)   # the atomic facts are the same for all the datasets\n",
    "ood_atomic_facts_ds = choose(ood_atomic_facts, test_size)  # the atomic facts are the same for all the datasets\n",
    "# here change\n",
    "\n",
    "t_3_test_inferred_iid_ds = choose(t_3_test_inferred_iid, test_size) #IID\n",
    "t_3_test_inferred_facts_ds = choose(t_3_test_inferred_facts, test_size) #OOD\n",
    "\n",
    "all_atomics = id_atomic_facts + ood_atomic_facts\n",
    "len(all_atomics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"3_HOP_composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_mask_NEW_correct/{}\".format(dataset_name), exist_ok=True)\n",
    "    t_3_train_inferred_facts_ds = choose(t_3_train_inferred_facts, round(phi * len(id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    \n",
    "    probes_3 = []\n",
    "    for item in id_atomic_facts_ds:\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1][\"type\"] = \"id_atomic\"\n",
    "    \n",
    "    for item in ood_atomic_facts_ds:\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    ######### change here for 3 hope \n",
    "\n",
    "    for item in choose(t_3_train_inferred_facts_ds, test_size):\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in t_3_test_inferred_iid:\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in t_3_test_inferred_facts_ds:\n",
    "        probes_3.append(deepcopy(item))\n",
    "        probes_3[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(all_atomics + t_3_train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(t_3_test_inferred_facts_ds, f)\n",
    "\n",
    "    # my validation with the test_inferred_iid    \n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(t_3_test_inferred_iid_ds, f)\n",
    "\n",
    "    #test.json = probes_3 \n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(probes_3, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 HOP split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size= 300  \n",
    "# same atomic as for split 2\n",
    "SPLIT_id_atomic_facts_ds = choose(SPLIT_id_atomic_facts, test_size)\n",
    "SPLIT_ood_atomic_facts_ds = choose(SPLIT_ood_atomic_facts, test_size)\n",
    "# here change from 2 split\n",
    "SPLIT_t3_HOP_test_inferred_iid_ds = choose(SPLIT_t3_HOP_test_inferred_iid, test_size) #IID\n",
    "SPLIT_t3_HOP_test_inferred_facts_ds = choose(SPLIT_t3_HOP_test_inferred_facts, test_size) #OOD\n",
    "\n",
    "SPLIT_t3_HOP_all_atomics =  SPLIT_id_atomic_facts + SPLIT_ood_atomic_facts\n",
    "len(SPLIT_t3_HOP_all_atomics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downsampling train_inferred\n",
    "\n",
    "for phi in [18.0,12.6,9.0,7.2,5.4,3.6]:  #phi is the ratio (Inferred facts / Atomic facts)\n",
    "    dataset_name = \"SPLIT_3_HOP_composition.{}.{}.{}\".format(NUM_ENTITY_IN, NUM_RELATION, phi)\n",
    "    os.makedirs(\"data_MIO_mask_NEW_correct/{}\".format(dataset_name), exist_ok=True)\n",
    "    SPLIT_t3_HOP_train_inferred_facts_ds = choose(SPLIT_t3_HOP_train_inferred_facts, round(phi * len(SPLIT_id_atomic_facts))) #downsampling the train_inferred_facts (ID)\n",
    "\n",
    "    SPLIT_t3_HOP_probes = []\n",
    "    for item in SPLIT_id_atomic_facts_ds:\n",
    "        SPLIT_t3_HOP_probes.append(deepcopy(item))\n",
    "        SPLIT_t3_HOP_probes[-1][\"type\"] = \"id_atomic\"\n",
    "\n",
    "    for item in SPLIT_ood_atomic_facts_ds:\n",
    "        SPLIT_t3_HOP_probes.append(deepcopy(item))\n",
    "        SPLIT_t3_HOP_probes[-1][\"type\"] = \"ood_atomic\"\n",
    "\n",
    "    ######### change here for 3 hope \n",
    "\n",
    "    for item in choose(SPLIT_t3_HOP_train_inferred_facts_ds, test_size):\n",
    "        SPLIT_t3_HOP_probes.append(deepcopy(item))\n",
    "        SPLIT_t3_HOP_probes[-1]['type'] = 'train_inferred'\n",
    "\n",
    "    for item in SPLIT_t3_HOP_test_inferred_iid:\n",
    "        SPLIT_t3_HOP_probes.append(deepcopy(item))\n",
    "        SPLIT_t3_HOP_probes[-1]['type'] = 'test_inferred_iid'\n",
    "\n",
    "    for item in SPLIT_t3_HOP_test_inferred_facts_ds:\n",
    "        SPLIT_t3_HOP_probes.append(deepcopy(item))\n",
    "        SPLIT_t3_HOP_probes[-1][\"type\"] = \"test_inferred_ood\"\n",
    "\n",
    "    # save the dataset Train, Test, Valid\n",
    "    #train.json = all_atomics + train_inferred_facts_ds\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/train.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(SPLIT_t3_HOP_all_atomics + SPLIT_t3_HOP_train_inferred_facts_ds, f)\n",
    "    #valid.json (only test_inferred_ OOD  why??? for what is that good?) \n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/valid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(SPLIT_t3_HOP_test_inferred_facts_ds, f)\n",
    "    \n",
    "    # my validation with the test_inferred_iid\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/valid_iid.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(SPLIT_t3_HOP_test_inferred_iid_ds, f)\n",
    "    \n",
    "    #test.json =\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/test.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(SPLIT_t3_HOP_probes, f)\n",
    "    # all vocab in the dataset (entities and relations) + special tokens\n",
    "    with open(\"data_MIO_mask_NEW_correct/{}/vocab.json\".format(dataset_name), \"w\", encoding='utf-8') as f:\n",
    "        json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_transformers_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
