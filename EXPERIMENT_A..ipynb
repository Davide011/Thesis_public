{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/s220331/.conda/envs/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/s220331/GROK/Thesis/transformers/src/transformers/utils/generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/s220331/GROK/Thesis/transformers/src/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "/home/s220331/GROK/Thesis/transformers/src/transformers/utils/generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Import the necessary functions\n",
    "import logging\n",
    "# Set the logging level to WARNING to suppress INFO messages\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "import transformers\n",
    "\n",
    "#logging.set_verbosity_error()\n",
    "# Disable specific warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "from inference_utils import load_model_and_tokenizer, generate_predictions, setup_device\n",
    "import json #, jsonlines\n",
    "\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "# Define the parameters for generation\n",
    "max_length = 10  # Adjust the max_length as needed\n",
    "num_return_sequences = 1  # Adjust the number of return sequence\n",
    "\n",
    "#predictions = generate_predictions(model, tokenizer, input_texts, device, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "def predict(model, tokenizer, input_texts, device, max_length=max_length, num_return_sequences=num_return_sequences):\n",
    "    predictions = generate_predictions(model, tokenizer, input_texts, device, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "    # Print the predictions\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        print(f\"Input: {input_texts[i]}\")\n",
    "        print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To chose the \"B\" that compare 2 times both in ID and OOD**\n",
    "def chose_B(range_):\n",
    "    list_ID=[]\n",
    "    list_OOD=[]\n",
    "    list_inferred_ID=[]\n",
    "    list_inferred_OOD=[]\n",
    "\n",
    "    count=0\n",
    "    for i in range(range_):\n",
    "        target = f'<e_{i}></a>'\n",
    "        t_print=f'<e_{i}>'\n",
    "        filtered_texts = [entry['target_text'] for entry in d['id_atomic'] if entry['target_text'].endswith(target)]\n",
    "\n",
    "        filtered_texts_OOD = [entry['target_text'] for entry in d['ood_atomic'] if entry['target_text'].endswith(target)]\n",
    "        if count <=30:\n",
    "            if len(filtered_texts) >= 2 and len(filtered_texts_OOD) >= 2:\n",
    "\n",
    "                filtered_texts_2_hop = [entry['target_text'] for entry in d['id_atomic'] if entry['target_text'].startswith(t_print)]\n",
    "\n",
    "                filtered_texts_OOD_2_hop = [entry['target_text'] for entry in d['ood_atomic'] if entry['target_text'].startswith(t_print)]\n",
    "\n",
    "                # Extract the part before <e_57></a> from the first element\n",
    "                #atomic_part_1_id = filtered_texts[0].split(target)[0]\n",
    "                #print(\"atomic_id\", atomic_part_1_id)\n",
    "                #atomic_part_1_ood = str(filtered_texts_OOD[0].split(target)[0])\n",
    "\n",
    "                #inferred_OOD = [entry['target_text'] for entry in d['test_inferred_ood']\n",
    "                #                    if entry['type'] == 'test_inferred_ood' and entry['input_text'].startswith(atomic_part_1_ood)]\n",
    "\n",
    "                #inferred_ID = [entry['target_text'] for entry in d['test_inferred_iid']\n",
    "                #                    if entry['type'] == 'test_inferred_iid' and entry['input_text'].startswith(atomic_part_1_id)]\n",
    "\n",
    "                if len(filtered_texts_2_hop) >= 1 and len(filtered_texts_OOD_2_hop) >= 1:\n",
    "                    \n",
    "\n",
    "                    list_ID.append(filtered_texts)\n",
    "                    list_OOD.append(filtered_texts_OOD)\n",
    "                    list_inferred_ID.append(filtered_texts_2_hop)\n",
    "                    list_inferred_OOD.append(filtered_texts_OOD_2_hop,)\n",
    "\n",
    "                    if count <=3:\n",
    "                        print(\"-------Target :\", t_print,  \"     ----------------------------------------------------------\\n\")\n",
    "                        print(\"ID\",filtered_texts   ,\"\\n\")\n",
    "                        print(\"OOD\",filtered_texts_OOD   ,\"\\n \\n\")\n",
    "\n",
    "                        #print(\"Inferred _ ID\", inferred_ID)\n",
    "                        #print(\"Inferred _ OOD\", inferred_OOD)\n",
    "                        # for the second hop\n",
    "\n",
    "                        count+=1\n",
    "                        print(\"2- hop  ID\", filtered_texts_2_hop)\n",
    "                        print(\"2nd hop OOD\", filtered_texts_OOD_2_hop, \"\\n \\n \\n\")\n",
    "                        \n",
    "    return list_ID, list_OOD, list_inferred_ID, list_inferred_OOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rank(hd, word_embedding_, token, metric='dot', token_list=None):\n",
    "    \"\"\"\n",
    "hd: Hidden states or output from a neural network.\n",
    "word_embedding_: Embedding matrix for words. (matrix that convert words to their embedding representation)\n",
    "token: The specific token (word) for which we want to find the rank.\n",
    "metric: The similarity metric to use ('dot' for dot product, 'cos' for cosine similarity).\n",
    "token_list: Optional list of tokens to consider for ranking.\"\"\"\n",
    "\n",
    "    if metric == 'dot':\n",
    "        word_embedding = word_embedding_\n",
    "    elif metric == 'cos':\n",
    "        word_embedding = F.normalize(word_embedding_, p=2, dim=1)\n",
    "    else:\n",
    "        assert False\n",
    "    #Compute the similarity scores (logits) between the hidden states (hd) and the word embeddings using matrix multiplication.\n",
    "    logits_ = torch.matmul(hd, word_embedding.T)  # a vlaue higher if he similarity with the analyzed \"word\" is higehr\n",
    "\n",
    "    rank = [] \n",
    "    for j in range(len(logits_)):\n",
    "        log = logits_[j].cpu().numpy()\n",
    "        if token_list is None:\n",
    "            temp = [[i, log[i]] for i in range(len(log))]\n",
    "        else:\n",
    "            temp = [[i, log[i]] for i in token_list]\n",
    "        temp.sort(key=lambda var: var[1], reverse=True)\n",
    "        rank.append([var[0] for var in temp].index(token))\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "def return_rank_norm_layer(hd, word_embedding_, token, metric='dot', token_list=None, layer_norm=None):\n",
    "    \"\"\"\n",
    "    Returns the rank of a given token based on similarity between hidden states and word embeddings.\n",
    "\n",
    "    Args:\n",
    "        hd (torch.Tensor): Hidden states of shape (batch_size, hidden_dim).\n",
    "        word_embedding_ (torch.Tensor): Word embedding matrix of shape (vocab_size, hidden_dim).\n",
    "        token (int): Target token index whose rank is to be found.\n",
    "        metric (str): Similarity metric ('dot' or 'cos'). Defaults to 'dot'.\n",
    "        token_list (list, optional): List of token indices to consider. Defaults to None.\n",
    "        layer_norm (LayerNorm, optional): LayerNorm instance for final layer normalization. \n",
    "                                          If None, no normalization is applied. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: Ranks of the target token for each instance in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply final layer normalization if provided\n",
    "    if layer_norm is not None:\n",
    "        hd = layer_norm(hd)\n",
    "\n",
    "    # Normalize word embeddings if metric is cosine similarity\n",
    "    if metric == 'cos':\n",
    "        word_embedding = F.normalize(word_embedding_, p=2, dim=1)\n",
    "    elif metric == 'dot':\n",
    "        word_embedding = word_embedding_\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Use 'dot' or 'cos'.\")\n",
    "\n",
    "    # Compute logits by projecting hidden states onto the embedding matrix\n",
    "    logits_ = torch.matmul(hd, word_embedding.T)\n",
    "\n",
    "    # Calculate rank of the target token\n",
    "    rank = []\n",
    "    for log in logits_:\n",
    "        log_np = log.cpu().numpy()\n",
    "        candidates = [[i, log_np[i]] for i in (token_list if token_list is not None else range(len(log_np)))]\n",
    "        candidates.sort(key=lambda var: var[1], reverse=True)\n",
    "        rank.append([var[0] for var in candidates].index(token))\n",
    "\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = \"/home/s220331/GROK/Thesis/data/composition_SMALL.200.20.12.6\"\n",
    "dataset=\"/home/s220331/GROK/Thesis/data/composition.2000.200.12.6\"\n",
    "model_path = \"/dtu-compute/s220331/composition/outputs_BIG_new/checkpoint-1500000/\"\n",
    "#\"/dtu-compute/s220331/composition/outputs_BIG_new/checkpoint-1500000/\" \n",
    "\n",
    "target_layer = 8\n",
    "\n",
    "\n",
    "# model set up\n",
    "#device = torch.device('cuda:5')\n",
    "device = setup_device()\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "word_embedding = model.lm_head.weight.data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case to remove\n",
    "dataset = \"/home/s220331/GROK/Thesis/data/composition_SMALL.200.20.12.6\"\n",
    "#dataset=\"/home/s220331/GROK/Thesis/data/composition.2000.200.12.6\"  #\"/dtu-compute/s220331/composition/outputs_BIG_new/checkpoint-1500000/\"\n",
    "model_path = \"/scratch/davide/model_paper/outputs_SMALL/checkpoint-1500000/\" # model normal on small dataset wuth chosen checkpoints!\n",
    "target_layer = 8\n",
    "\n",
    "\n",
    "# model set up\n",
    "#device = torch.device('cuda:5')\n",
    "device = setup_device()\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "word_embedding = model.lm_head.weight.data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51880/51880 [00:00<00:00, 1405034.43it/s]\n",
      "100%|██████████| 51880/51880 [00:00<00:00, 627269.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# id_atomic, # ood_atomic: 3800 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set data\n",
    "\n",
    "all_atomic = set()     # (h,r,t)\n",
    "atomic_dict = dict()   # (h,r) -> t\n",
    "with open(dataset+\"/train.json\") as f:  # from the correct data or data_MIO !!!\n",
    "    train_items = json.load(f)\n",
    "for item in tqdm(train_items):\n",
    "    temp = item['target_text'].strip(\"><\").split(\"><\")\n",
    "    if len(temp) != 4:\n",
    "        continue\n",
    "    h,r,t = temp[:3]\n",
    "    atomic_dict[(h,r)] = t\n",
    "    all_atomic.add((h,r,t))\n",
    "\n",
    "id_atomic = set()\n",
    "for item in tqdm(train_items):\n",
    "    temp = item['target_text'].strip(\"><\").split(\"><\")\n",
    "    if len(temp) == 4:\n",
    "        continue\n",
    "    h, r1, r2, t = temp[:4]\n",
    "    b = atomic_dict[(h, r1)]\n",
    "    assert atomic_dict[(b, r2)] == t\n",
    "    id_atomic.add((h,r1,b))\n",
    "    id_atomic.add((b,r2,t))\n",
    "\n",
    "ood_atomic = all_atomic - id_atomic\n",
    "print(\"# id_atomic, # ood_atomic:\", len(id_atomic), len(ood_atomic))\n",
    "\n",
    "# smart way to save all the train\n",
    "with open(dataset+\"/test.json\") as f:\n",
    "    pred_data = json.load(f)\n",
    "d = dict()\n",
    "for item in pred_data:\n",
    "    t = item['type']\n",
    "    if t not in d:\n",
    "        d[t] = []\n",
    "    d[t].append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To chose the \"B\" that compare 2 times both in ID and OOD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Target : <e_11>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_150><r_11><e_11></a>', '<e_140><r_19><e_11></a>', '<e_128><r_3><e_11></a>'] \n",
      "\n",
      "OOD ['<e_147><r_4><e_11></a>', '<e_9><r_16><e_11></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_11><r_11><e_88></a>']\n",
      "2nd hop OOD ['<e_11><r_1><e_141></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_12>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_38><r_7><e_12></a>', '<e_195><r_5><e_12></a>', '<e_122><r_8><e_12></a>'] \n",
      "\n",
      "OOD ['<e_136><r_2><e_12></a>', '<e_1><r_16><e_12></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_12><r_4><e_21></a>']\n",
      "2nd hop OOD ['<e_12><r_2><e_5></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_47>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_117><r_4><e_47></a>', '<e_32><r_8><e_47></a>', '<e_130><r_3><e_47></a>', '<e_79><r_3><e_47></a>'] \n",
      "\n",
      "OOD ['<e_105><r_9><e_47></a>', '<e_6><r_1><e_47></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_47><r_15><e_180></a>']\n",
      "2nd hop OOD ['<e_47><r_7><e_13></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_114>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_181><r_6><e_114></a>', '<e_84><r_13><e_114></a>'] \n",
      "\n",
      "OOD ['<e_123><r_8><e_114></a>', '<e_14><r_12><e_114></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_114><r_5><e_137></a>']\n",
      "2nd hop OOD ['<e_114><r_13><e_117></a>'] \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "a= chose_B(199)\n",
    "#a[1][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_change_OOD =\"<e_147><r_4><r_1>\" #\"<e_181><r_6><r_5>\"# \"<e_147><r_4><r_1>\"# \"<e_14><r_12><r_13>\"       # 2 hop ['<e_11><r_1><e_141></a>'] \n",
    "\n",
    "def run_query(query_change_OOD, ood=False):\n",
    "    # inser here the 2-hop query (like ood) from whihc extract the first hop-- \"b\" activations\n",
    "    # usato \"<e_147><r_4><r_1>\"\n",
    "    h_1_ood,n_r_1_ood,r_1_ood = query_change_OOD.strip(\"><\").split(\"><\") #\"e_140\",\"r_19\",\"r_11\"             #query.strip(\"><\").split(\"><\")\n",
    "    b_1_ood = atomic_dict[(h_1_ood ,n_r_1_ood)]\n",
    "    t_1_ood = atomic_dict[(b_1_ood, r_1_ood)]\n",
    "    list_b_t=[]\n",
    "\n",
    "    print(\"b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \",b_1_ood)\n",
    "    print(\"tail:\",t_1_ood)\n",
    "\n",
    "\n",
    "    # Tokenizing the Query:\n",
    "    decoder_temp_1_OOD = tokenizer([query_change_OOD], return_tensors=\"pt\", padding=True)\n",
    "    decoder_input_ids_1_OOD, decoder_attention_mask_1_OOD = decoder_temp_1_OOD[\"input_ids\"], decoder_temp_1_OOD[\"attention_mask\"]\n",
    "    decoder_input_ids_1_OOD, decoder_attention_mask_1_OOD = decoder_input_ids_1_OOD.to(device), decoder_attention_mask_1_OOD.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # here the same model but with different input !! \n",
    "        outputs_1_OOD = model(\n",
    "            input_ids=decoder_input_ids_1_OOD,\n",
    "            attention_mask=decoder_attention_mask_1_OOD,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    # hidden state of the 1_id (which will be inserted in the normal run)\n",
    "    all_hidden_states_1_OOD = outputs_1_OOD['hidden_states']\n",
    "\n",
    "    #################### just to quick check everything is right\n",
    "\n",
    "    res_dict_b_1_OOD = dict()\n",
    "    target_layer_b = 4\n",
    "    target_layer_t = 8\n",
    "    # (\"<\"+ b +\">\")  is just for reconstructing (\"<b>\") from \"b\"\n",
    "    rank_before_1_OOD = return_rank_norm_layer(all_hidden_states_1_OOD[target_layer_b][0, :, :], word_embedding, tokenizer(\"<\"+ b_1_ood +\">\")['input_ids'][0])[-2]\n",
    "    res_dict_b_1_OOD['rank_before, layer 5 b'] = rank_before_1_OOD\n",
    "\n",
    "    # laste layer search for t\n",
    "    rank_before_1_OOD_t = return_rank_norm_layer(all_hidden_states_1_OOD[target_layer_t][0, :, :], word_embedding, tokenizer(\"<\"+ t_1_ood +\">\")['input_ids'][0])[-1]\n",
    "    res_dict_b_1_OOD['rank_before, layer 8 search t'] = rank_before_1_OOD_t\n",
    "    list_b_t.append((rank_before_1_OOD, rank_before_1_OOD_t))\n",
    "    if ood:\n",
    "        print(\"OOD\")\n",
    "        print(\" B 1_OOD\",res_dict_b_1_OOD)\n",
    "        print(\"as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\")\n",
    "    else:\n",
    "        print(\"ID\")\n",
    "        print(\" B 1_OOD\",res_dict_b_1_OOD)\n",
    "        print(\"as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\")\n",
    "\n",
    "    return all_hidden_states_1_OOD , b_1_ood ,t_1_ood ,list_b_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict_OOD_intervention= dict()\n",
    "def replace_hidden_state(hidden_to_insert,b, all_hidden_states_to_change,t):\n",
    "    list_intervention_results = []\n",
    "    target_layer_intervention = 4\n",
    "    target_layer_final = 8\n",
    "    # juest rename for more standardized structure\n",
    "    all_hidden_states_ctft_1 = hidden_to_insert\n",
    "\n",
    "    # we are analysing  with replacing only specific hidden state once a time\n",
    "    for layer_to_intervene in range(target_layer_intervention , target_layer_intervention +1):\n",
    "        print(layer_to_intervene)\n",
    "        hidden_states_1 = all_hidden_states_to_change[layer_to_intervene].clone()    # cloniamo gli hidden del normal run\n",
    "        hidden_states_ctft_1 = all_hidden_states_ctft_1[layer_to_intervene]  # copiamo quelli del run che inseriremo\n",
    "    #r_2  r\n",
    "        #r_2_ood = all_hidden_states_1_OOD_test[layer_to_intervene]\n",
    "\n",
    "        # intervene\n",
    "        hidden_states_1[0, 0, :] = hidden_states_ctft_1[0, 0, :]   #insereiamo gli hidden state che vogliamo\n",
    "        hidden_states_1[0, 1, :] = hidden_states_ctft_1[0, 1, :]\n",
    "        #hidden_states_1[0, 2, :] = hidden_states_ctft_1[0, 2, :]  # prova ad inserire r_2 di ood\n",
    "        #hidden_states_1[0, 2, :]= r_11_id\n",
    "        #hidden_states_1[0, 2, :]= r_2_ood[0, 2, :] # this is a r(second hop) coming from another ood (with not even a;r_1 the same. This prove that r_2 does not \n",
    "        # store calculation in the first 5 layers according to paper results.)\n",
    "        \n",
    "\n",
    "        rank_middle_1 = return_rank_norm_layer(hidden_states_1[0, :, :], word_embedding, tokenizer(\"<\"+b+\">\")['input_ids'][0])[-2]\n",
    "        res_dict_OOD_intervention[\"insertion at layer\"+str(layer_to_intervene), \"B\"] = rank_middle_1\n",
    "        #                \n",
    "        with torch.no_grad():\n",
    "            for i in range(layer_to_intervene, target_layer_final):\n",
    "                f_layer_1 = model.transformer.h[i]  # current layer\n",
    "                print(\"layer aggiornato:\",i)\n",
    "                # attention mechanism \n",
    "                residual_1 = hidden_states_1       #store the hidden state in residual\n",
    "                # Apply layer normalization to hidden_states.\n",
    "                hidden_states_1 = f_layer_1.ln_1(hidden_states_1)\n",
    "                attn_output_1 = f_layer_1.attn(hidden_states_1)[0] \n",
    "                hidden_states_1 = attn_output_1 + residual_1\n",
    "                # mlp  = Feed-Forward Network:\n",
    "                residual_1 = hidden_states_1\n",
    "                hidden_states_1 = f_layer_1.ln_2(hidden_states_1)\n",
    "                feed_forward_hidden_states_1 = f_layer_1.mlp.c_proj(f_layer_1.mlp.act(f_layer_1.mlp.c_fc(hidden_states_1)))\n",
    "                hidden_states_1 = residual_1 + feed_forward_hidden_states_1\n",
    "            # final ln\n",
    "            hidden_states_1 = model.transformer.ln_f(hidden_states_1)\n",
    "        # print(\"--------\")\n",
    "        rank_after_1 = return_rank(hidden_states_1[0, :, :], word_embedding, tokenizer(\"<\"+t+\">\")['input_ids'][0])[-1]\n",
    "        res_dict_OOD_intervention[\"Out-tput layer\"+str(target_layer_final), \"t\"] = rank_after_1\n",
    "        list_intervention_results.append(rank_middle_1)\n",
    "        list_intervention_results.append(rank_after_1)\n",
    "        print(\"intervention results\",res_dict_OOD_intervention)\n",
    "    print( list_intervention_results)\n",
    "    return list_intervention_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Intervention with OOD insertion**\n",
    "\n",
    "What: Insert in the normal run (ID run) the 5 layer hydden layers coming from a OOD run representing the same Bridge entity and r_2 relation.\n",
    "It has be seen that the model still predict B (as expected as also the OOD predict the B) but also the t. In other words, does not seem that the model store and use information about how he gets to the Bridge. Given one Bridge he is able to secondo hop if the second hop is in ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Target : <e_11>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_150><r_11><e_11></a>', '<e_140><r_19><e_11></a>', '<e_128><r_3><e_11></a>'] \n",
      "\n",
      "OOD ['<e_147><r_4><e_11></a>', '<e_9><r_16><e_11></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_11><r_11><e_88></a>']\n",
      "2nd hop OOD ['<e_11><r_1><e_141></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_12>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_38><r_7><e_12></a>', '<e_195><r_5><e_12></a>', '<e_122><r_8><e_12></a>'] \n",
      "\n",
      "OOD ['<e_136><r_2><e_12></a>', '<e_1><r_16><e_12></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_12><r_4><e_21></a>']\n",
      "2nd hop OOD ['<e_12><r_2><e_5></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_47>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_117><r_4><e_47></a>', '<e_32><r_8><e_47></a>', '<e_130><r_3><e_47></a>', '<e_79><r_3><e_47></a>'] \n",
      "\n",
      "OOD ['<e_105><r_9><e_47></a>', '<e_6><r_1><e_47></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_47><r_15><e_180></a>']\n",
      "2nd hop OOD ['<e_47><r_7><e_13></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_114>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_181><r_6><e_114></a>', '<e_84><r_13><e_114></a>'] \n",
      "\n",
      "OOD ['<e_123><r_8><e_114></a>', '<e_14><r_12><e_114></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_114><r_5><e_137></a>']\n",
      "2nd hop OOD ['<e_114><r_13><e_117></a>'] \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "atomic_to_chose = chose_B(199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_ID--------------------------------------------------------------------------- <e_140><r_19><r_11>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_147><r_4><r_1>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_11\n",
      "tail: e_88\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_11\n",
      "tail: e_141\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 156}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_11\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_195><r_5><r_4>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_136><r_2><r_2>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_12\n",
      "tail: e_21\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_12\n",
      "tail: e_5\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 83}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_12\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_32><r_8><r_15>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_105><r_9><r_7>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_47\n",
      "tail: e_180\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_47\n",
      "tail: e_13\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 118}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_47\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_84><r_13><r_5>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_123><r_8><r_13>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_114\n",
      "tail: e_137\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_114\n",
      "tail: e_117\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 43}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_114\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_137><r_3><r_19>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_3><r_5><r_13>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_125\n",
      "tail: e_60\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_125\n",
      "tail: e_100\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 114}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_125\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_194><r_5><r_14>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_149><r_13><r_15>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_137\n",
      "tail: e_121\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_137\n",
      "tail: e_144\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 64}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_137\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_132><r_15><r_3>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_99><r_19><r_19>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_151\n",
      "tail: e_142\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_151\n",
      "tail: e_67\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 79}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_151\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# take 1 from 1_id and 1 from inferred to construct the query\n",
    "ID_ID_list = []\n",
    "OOD_OOD_list = []\n",
    "OOD_intervention_in_ID= []\n",
    "for i in range(0,7):\n",
    "    _1_id = atomic_to_chose[0][i]  # list with all the\n",
    "    _1_ood = atomic_to_chose[1][i]\n",
    "    _1_id_inferred = atomic_to_chose[2][i]\n",
    "    _1_ood_inferred = atomic_to_chose[3][i]\n",
    "\n",
    "    text_1 = _1_id[1]\n",
    "    match = re.match(r'^(<e_\\d+><r_\\d+>)', text_1)\n",
    "    if match:\n",
    "        result_1_id = match.group(1)\n",
    "    text_2= _1_id_inferred[0]\n",
    "    match = re.match(r'^<e_\\d+>(<r_\\d+>)', text_2)\n",
    "    if match:\n",
    "        result2_id = match.group(1)\n",
    "\n",
    "    text_3 = _1_ood[0]\n",
    "    match = re.match(r'^(<e_\\d+><r_\\d+>)', text_3)\n",
    "    if match:\n",
    "        result_1_ood = match.group(1)\n",
    "    text_4= _1_ood_inferred[0]\n",
    "    match = re.match(r'^<e_\\d+>(<r_\\d+>)', text_4)\n",
    "    if match:\n",
    "        result2_ood = match.group(1)\n",
    "\n",
    "    id_id = result_1_id + result2_id\n",
    "    ood_ood = result_1_ood + result2_ood\n",
    "    print(\"ID_ID---------------------------------------------------------------------------\", id_id)   \n",
    "    print(\"OOD_OOD------------------------------------------------------------------------------------\", ood_ood)\n",
    "    normal_run_ID_ID =run_query(id_id, ood=False)\n",
    "    B_OOD_insert_activations =run_query(ood_ood, ood=True) \n",
    "    print(\"-------------------------------------------------------------------------------------------------------------------b\",B_OOD_insert_activations[1])\n",
    "    \n",
    "    intervention=  replace_hidden_state(B_OOD_insert_activations[0],B_OOD_insert_activations[1],normal_run_ID_ID[0], normal_run_ID_ID[2])  # hidden to insert, b, (all hidden states) normal run, t\n",
    "    \n",
    "    ID_ID_list.append(normal_run_ID_ID[3])\n",
    "    OOD_OOD_list.append(B_OOD_insert_activations[3])\n",
    "    OOD_intervention_in_ID.append(intervention)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(atomic_to_chose[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 156)],\n",
       " [(0, 83)],\n",
       " [(0, 118)],\n",
       " [(0, 43)],\n",
       " [(0, 114)],\n",
       " [(0, 64)],\n",
       " [(0, 79)]]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOD_OOD_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0], [0, 0]]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOD_intervention_in_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)]]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_ID_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PURE B fro layer 0 to lyer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict_OOD_intervention_PURE__B= dict()\n",
    "def replace_hidden_state_B(hidden_to_insert,b, all_hidden_states_to_change,t):\n",
    "    list_intervention_results = []\n",
    "    target_layer_intervention = 4\n",
    "    target_layer_final = 8\n",
    "    target_B=0\n",
    "    # juest rename for more standardized structure\n",
    "    all_hidden_states_ctft_1 = hidden_to_insert\n",
    "\n",
    "    # we are analysing  with replacing only specific hidden state once a time\n",
    "    for layer_to_intervene in range(target_layer_intervention , target_layer_intervention +1):\n",
    "        print(layer_to_intervene)\n",
    "        hidden_states_1 = all_hidden_states_to_change[layer_to_intervene].clone()    # cloniamo gli hidden del normal run\n",
    "        hidden_states_ctft_1 = all_hidden_states_ctft_1[target_B]  # copiamo quelli del run che inseriremo\n",
    "    #r_2  r\n",
    "        #r_2_ood = all_hidden_states_1_OOD_test[layer_to_intervene]\n",
    "\n",
    "        # intervene\n",
    "        #hidden_states_1[0, 0, :] = hidden_states_ctft_1[0, 0, :]   #insereiamo gli hidden state che vogliamo\n",
    "        hidden_states_1[0, 1, :] = hidden_states_ctft_1[0, 2, :]\n",
    "        hidden_states_1[0, 2, :] = hidden_states_ctft_1[0, 1, :]  # prova ad inserire r_2 di ood\n",
    "        #hidden_states_1[0, 2, :]= r_11_id\n",
    "        #hidden_states_1[0, 2, :]= r_2_ood[0, 2, :] # this is a r(second hop) coming from another ood (with not even a;r_1 the same. This prove that r_2 does not \n",
    "        # store calculation in the first 5 layers according to paper results.)\n",
    "        \n",
    "\n",
    "        rank_middle_1 = return_rank_norm_layer(hidden_states_1[0, :, :], word_embedding, tokenizer(\"<\"+b+\">\")['input_ids'][0])[-2]\n",
    "        res_dict_OOD_intervention[\"insertion at layer\"+str(layer_to_intervene), \"B\"] = rank_middle_1\n",
    "        #                \n",
    "        with torch.no_grad():\n",
    "            for i in range(layer_to_intervene, target_layer_final):\n",
    "                f_layer_1 = model.transformer.h[i]  # current layer\n",
    "                print(\"layer aggiornato:\",i)\n",
    "                # attention mechanism \n",
    "                residual_1 = hidden_states_1       #store the hidden state in residual\n",
    "                # Apply layer normalization to hidden_states.\n",
    "                hidden_states_1 = f_layer_1.ln_1(hidden_states_1)\n",
    "                attn_output_1 = f_layer_1.attn(hidden_states_1)[0] \n",
    "                hidden_states_1 = attn_output_1 + residual_1\n",
    "                # mlp  = Feed-Forward Network:\n",
    "                residual_1 = hidden_states_1\n",
    "                hidden_states_1 = f_layer_1.ln_2(hidden_states_1)\n",
    "                feed_forward_hidden_states_1 = f_layer_1.mlp.c_proj(f_layer_1.mlp.act(f_layer_1.mlp.c_fc(hidden_states_1)))\n",
    "                hidden_states_1 = residual_1 + feed_forward_hidden_states_1\n",
    "            # final ln\n",
    "            hidden_states_1 = model.transformer.ln_f(hidden_states_1)\n",
    "        # print(\"--------\")\n",
    "        rank_after_1 = return_rank_norm_layer(hidden_states_1[0, :, :], word_embedding, tokenizer(\"<\"+t+\">\")['input_ids'][0])[-1]\n",
    "        res_dict_OOD_intervention[\"Out-tput layer\"+str(target_layer_final), \"t\"] = rank_after_1\n",
    "        list_intervention_results.append(rank_middle_1)\n",
    "        list_intervention_results.append(rank_after_1)\n",
    "        print(\"intervention results\",res_dict_OOD_intervention)\n",
    "    print( list_intervention_results)\n",
    "    return list_intervention_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_change_OOD =\"<e_147><r_4><r_1>\" #\"<e_181><r_6><r_5>\"# \"<e_147><r_4><r_1>\"# \"<e_14><r_12><r_13>\"       # 2 hop ['<e_11><r_1><e_141></a>'] \n",
    "\n",
    "def run_query_B(query_change_OOD, ood=False):\n",
    "    # inser here the 2-hop query (like ood) from whihc extract the first hop-- \"b\" activations\n",
    "    # usato \"<e_147><r_4><r_1>\"\n",
    "    h_1_ood,n_r_1_ood,r_1_ood = query_change_OOD.strip(\"><\").split(\"><\") #\"e_140\",\"r_19\",\"r_11\"             #query.strip(\"><\").split(\"><\")\n",
    "    b_1_ood = atomic_dict[(h_1_ood ,n_r_1_ood)]\n",
    "    t_1_ood = atomic_dict[(b_1_ood, r_1_ood)]\n",
    "    list_b_t=[]\n",
    "\n",
    "    print(\"b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \",b_1_ood)\n",
    "    print(\"tail:\",t_1_ood)\n",
    "    \n",
    "    query_B_layer_for_layer_0=  \"<\"+ h_1_ood +\">\" +\"<\"+ n_r_1_ood +\">\"+ \"<\"+ b_1_ood +\">\"\n",
    "\n",
    "    # Tokenizing the Query:\n",
    "    decoder_temp_1_OOD = tokenizer([query_B_layer_for_layer_0], return_tensors=\"pt\", padding=True)\n",
    "    decoder_input_ids_1_OOD, decoder_attention_mask_1_OOD = decoder_temp_1_OOD[\"input_ids\"], decoder_temp_1_OOD[\"attention_mask\"]\n",
    "    decoder_input_ids_1_OOD, decoder_attention_mask_1_OOD = decoder_input_ids_1_OOD.to(device), decoder_attention_mask_1_OOD.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # here the same model but with different input !! \n",
    "        outputs_1_OOD = model(\n",
    "            input_ids=decoder_input_ids_1_OOD,\n",
    "            attention_mask=decoder_attention_mask_1_OOD,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    # hidden state of the 1_id (which will be inserted in the normal run)\n",
    "    all_hidden_states_1_OOD = outputs_1_OOD['hidden_states']\n",
    "\n",
    "    #################### just to quick check everything is right\n",
    "\n",
    "    res_dict_b_1_OOD = dict()\n",
    "    target_layer_b = 4\n",
    "    target_layer_t = 8\n",
    "    # (\"<\"+ b +\">\")  is just for reconstructing (\"<b>\") from \"b\"\n",
    "    rank_before_1_OOD = return_rank_norm_layer(all_hidden_states_1_OOD[target_layer_b][0, :, :], word_embedding, tokenizer(\"<\"+ b_1_ood +\">\")['input_ids'][0])[-2]\n",
    "    res_dict_b_1_OOD['rank_before, layer 5 b'] = rank_before_1_OOD\n",
    "\n",
    "    # laste layer search for t\n",
    "    rank_before_1_OOD_t = return_rank_norm_layer(all_hidden_states_1_OOD[target_layer_t][0, :, :], word_embedding, tokenizer(\"<\"+ t_1_ood +\">\")['input_ids'][0])[-1]\n",
    "    res_dict_b_1_OOD['rank_before, layer 8 search t'] = rank_before_1_OOD_t\n",
    "    list_b_t.append((rank_before_1_OOD, rank_before_1_OOD_t))\n",
    "    if ood:\n",
    "        print(\"OOD\")\n",
    "        print(\" B 1_OOD\",res_dict_b_1_OOD)\n",
    "        print(\"as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\")\n",
    "    else:\n",
    "        print(\"ID\")\n",
    "        print(\" B 1_OOD\",res_dict_b_1_OOD)\n",
    "        print(\"as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\")\n",
    "\n",
    "    return all_hidden_states_1_OOD , b_1_ood ,t_1_ood ,list_b_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_ID--------------------------------------------------------------------------- <e_150><r_11><r_11>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_147><r_4><r_1>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_11\n",
      "tail: e_88\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_11\n",
      "tail: e_88\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 107}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_151\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 127}\n",
      "[0, 127]\n",
      "ID_ID--------------------------------------------------------------------------- <e_38><r_7><r_4>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_136><r_2><r_2>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_12\n",
      "tail: e_21\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_12\n",
      "tail: e_21\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 10}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_151\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 119}\n",
      "[0, 119]\n",
      "ID_ID--------------------------------------------------------------------------- <e_117><r_4><r_15>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_105><r_9><r_7>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_47\n",
      "tail: e_180\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_47\n",
      "tail: e_180\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_151\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 17}\n",
      "[0, 17]\n",
      "ID_ID--------------------------------------------------------------------------- <e_181><r_6><r_5>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_123><r_8><r_13>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_114\n",
      "tail: e_137\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_114\n",
      "tail: e_137\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 26}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_151\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 77}\n",
      "[0, 77]\n",
      "ID_ID--------------------------------------------------------------------------- <e_23><r_7><r_19>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_3><r_5><r_13>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_125\n",
      "tail: e_60\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_125\n",
      "tail: e_60\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 90}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_151\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 117}\n",
      "[0, 117]\n",
      "ID_ID--------------------------------------------------------------------------- <e_114><r_5><r_14>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_149><r_13><r_15>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_137\n",
      "tail: e_121\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_137\n",
      "tail: e_121\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 5}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_151\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 13}\n",
      "[0, 13]\n",
      "ID_ID--------------------------------------------------------------------------- <e_90><r_10><r_3>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_99><r_19><r_19>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_151\n",
      "tail: e_142\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_151\n",
      "tail: e_142\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 2}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_151\n",
      "4\n",
      "layer aggiornato: 4\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer4', 'B'): 0, ('Out-tput layer8', 't'): 2}\n",
      "[0, 2]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# take 1 from 1_id and 1 from inferred to construct the query\n",
    "ID_ID_list_P = []\n",
    "OOD_OOD_list_P = []\n",
    "OOD_intervention_in_ID_P= []\n",
    "for i in range(0,7):\n",
    "    _1_id = atomic_to_chose[0][i]  # list with all the\n",
    "    _1_ood = atomic_to_chose[1][i]\n",
    "    _1_id_inferred = atomic_to_chose[2][i]\n",
    "    _1_ood_inferred = atomic_to_chose[3][i]\n",
    "\n",
    "    text_1 = _1_id[0]\n",
    "    match = re.match(r'^(<e_\\d+><r_\\d+>)', text_1)\n",
    "    if match:\n",
    "        result_1_id = match.group(1)\n",
    "\n",
    "    text_1_1 = _1_id[1]\n",
    "    match_1 = re.match(r'^(<e_\\d+><r_\\d+>)', text_1_1)\n",
    "    if match_1:\n",
    "        result_1_id_1 = match_1.group(1)\n",
    "\n",
    "    \n",
    "    text_2= _1_id_inferred[0]\n",
    "    match = re.match(r'^<e_\\d+>(<r_\\d+>)', text_2)\n",
    "    if match:\n",
    "        result2_id = match.group(1)\n",
    "\n",
    "    text_3 = _1_ood[0]\n",
    "    match = re.match(r'^(<e_\\d+><r_\\d+>)', text_3)\n",
    "    if match:\n",
    "        result_1_ood = match.group(1)\n",
    "    text_4= _1_ood_inferred[0]\n",
    "    match = re.match(r'^<e_\\d+>(<r_\\d+>)', text_4)\n",
    "    if match:\n",
    "        result2_ood = match.group(1)\n",
    "\n",
    "    id_id = result_1_id + result2_id\n",
    "\n",
    "    id_id_1 = result_1_id_1 + result2_id\n",
    "    ood_ood = result_1_ood + result2_ood\n",
    "    print(\"ID_ID---------------------------------------------------------------------------\", id_id)   \n",
    "    print(\"OOD_OOD------------------------------------------------------------------------------------\", ood_ood)\n",
    "    normal_run_ID_ID =run_query(id_id, ood=False)\n",
    "    #B_OOD_insert_activations =run_query(ood_ood, ood=True) \n",
    "    normal_2 =run_query_B(id_id_1 , ood=True)\n",
    "    print(\"-------------------------------------------------------------------------------------------------------------------b\",B_OOD_insert_activations[1])\n",
    "    \n",
    "    intervention=  replace_hidden_state_B(normal_2[0],normal_2[1],normal_run_ID_ID[0], normal_run_ID_ID[2])  # hidden to insert, b, (all hidden states) normal run, t\n",
    "    #intervention=  replace_hidden_state_B(normal_2[0],normal_2[1],normal_run_ID_ID[0], normal_run_ID_ID[2])  # hidden to insert, b, (all hidden states) normal run, t\n",
    "    \n",
    "    ID_ID_list_P.append(normal_run_ID_ID[3])\n",
    "    OOD_OOD_list_P.append(normal_2[3])\n",
    "    OOD_intervention_in_ID_P.append(intervention)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)], [(0, 0)]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_ID_list_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 107)], [(0, 10)], [(0, 1)], [(0, 26)], [(0, 90)], [(0, 5)], [(0, 2)]]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOD_OOD_list_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 127], [0, 119], [0, 17], [0, 77], [0, 117], [0, 13], [0, 2]]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOD_intervention_in_ID_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
