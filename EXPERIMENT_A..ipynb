{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary functions\n",
    "import logging\n",
    "# Set the logging level to WARNING to suppress INFO messages\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "import transformers\n",
    "\n",
    "#logging.set_verbosity_error()\n",
    "# Disable specific warnings\n",
    "transformers.logging.set_verbosity_error()\n",
    "from inference_utils import load_model_and_tokenizer, generate_predictions, setup_device\n",
    "import json #, jsonlines\n",
    "\n",
    "from transformers import GPT2Config, GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "from tqdm import tqdm\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions\n",
    "# Define the parameters for generation\n",
    "max_length = 10  # Adjust the max_length as needed\n",
    "num_return_sequences = 1  # Adjust the number of return sequence\n",
    "\n",
    "#predictions = generate_predictions(model, tokenizer, input_texts, device, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "def predict(model, tokenizer, input_texts, device, max_length=max_length, num_return_sequences=num_return_sequences):\n",
    "    predictions = generate_predictions(model, tokenizer, input_texts, device, max_length=max_length, num_return_sequences=num_return_sequences)\n",
    "    # Print the predictions\n",
    "    for i, prediction in enumerate(predictions):\n",
    "        print(f\"Input: {input_texts[i]}\")\n",
    "        print(f\"Prediction: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To chose the \"B\" that compare 2 times both in ID and OOD**\n",
    "def chose_B(range_):\n",
    "    list_ID=[]\n",
    "    list_OOD=[]\n",
    "    list_inferred_ID=[]\n",
    "    list_inferred_OOD=[]\n",
    "\n",
    "    count=0\n",
    "    for i in range(range_):\n",
    "        target = f'<e_{i}></a>'\n",
    "        t_print=f'<e_{i}>'\n",
    "        filtered_texts = [entry['target_text'] for entry in d['id_atomic'] if entry['target_text'].endswith(target)]\n",
    "\n",
    "        filtered_texts_OOD = [entry['target_text'] for entry in d['ood_atomic'] if entry['target_text'].endswith(target)]\n",
    "        if count <=2000:\n",
    "            if len(filtered_texts) >= 1 and len(filtered_texts_OOD) >= 1:\n",
    "\n",
    "                filtered_texts_2_hop = [entry['target_text'] for entry in d['id_atomic'] if entry['target_text'].startswith(t_print)]\n",
    "\n",
    "                filtered_texts_OOD_2_hop = [entry['target_text'] for entry in d['ood_atomic'] if entry['target_text'].startswith(t_print)]\n",
    "\n",
    "                # Extract the part before <e_57></a> from the first element\n",
    "                #atomic_part_1_id = filtered_texts[0].split(target)[0]\n",
    "                #print(\"atomic_id\", atomic_part_1_id)\n",
    "                #atomic_part_1_ood = str(filtered_texts_OOD[0].split(target)[0])\n",
    "\n",
    "                #inferred_OOD = [entry['target_text'] for entry in d['test_inferred_ood']\n",
    "                #                    if entry['type'] == 'test_inferred_ood' and entry['input_text'].startswith(atomic_part_1_ood)]\n",
    "\n",
    "                #inferred_ID = [entry['target_text'] for entry in d['test_inferred_iid']\n",
    "                #                    if entry['type'] == 'test_inferred_iid' and entry['input_text'].startswith(atomic_part_1_id)]\n",
    "\n",
    "                if len(filtered_texts_2_hop) >= 1 and len(filtered_texts_OOD_2_hop) >= 1:\n",
    "                    \n",
    "\n",
    "                    list_ID.append(filtered_texts)\n",
    "                    list_OOD.append(filtered_texts_OOD)\n",
    "                    list_inferred_ID.append(filtered_texts_2_hop)\n",
    "                    list_inferred_OOD.append(filtered_texts_OOD_2_hop,)\n",
    "\n",
    "                    if count <=3:\n",
    "                        print(\"-------Target :\", t_print,  \"     ----------------------------------------------------------\\n\")\n",
    "                        print(\"ID\",filtered_texts   ,\"\\n\")\n",
    "                        print(\"OOD\",filtered_texts_OOD   ,\"\\n \\n\")\n",
    "\n",
    "                        #print(\"Inferred _ ID\", inferred_ID)\n",
    "                        #print(\"Inferred _ OOD\", inferred_OOD)\n",
    "                        # for the second hop\n",
    "\n",
    "                        count+=1\n",
    "                        print(\"2- hop  ID\", filtered_texts_2_hop)\n",
    "                        print(\"2nd hop OOD\", filtered_texts_OOD_2_hop, \"\\n \\n \\n\")\n",
    "                        \n",
    "    return list_ID, list_OOD, list_inferred_ID, list_inferred_OOD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_rank(hd, word_embedding_, token, metric='dot', token_list=None):\n",
    "    \"\"\"\n",
    "hd: Hidden states or output from a neural network.\n",
    "word_embedding_: Embedding matrix for words. (matrix that convert words to their embedding representation)\n",
    "token: The specific token (word) for which we want to find the rank.\n",
    "metric: The similarity metric to use ('dot' for dot product, 'cos' for cosine similarity).\n",
    "token_list: Optional list of tokens to consider for ranking.\"\"\"\n",
    "\n",
    "    if metric == 'dot':\n",
    "        word_embedding = word_embedding_\n",
    "    elif metric == 'cos':\n",
    "        word_embedding = F.normalize(word_embedding_, p=2, dim=1)\n",
    "    else:\n",
    "        assert False\n",
    "    #Compute the similarity scores (logits) between the hidden states (hd) and the word embeddings using matrix multiplication.\n",
    "    logits_ = torch.matmul(hd, word_embedding.T)  # a vlaue higher if he similarity with the analyzed \"word\" is higehr\n",
    "\n",
    "    rank = [] \n",
    "    for j in range(len(logits_)):\n",
    "        log = logits_[j].cpu().numpy()\n",
    "        if token_list is None:\n",
    "            temp = [[i, log[i]] for i in range(len(log))]\n",
    "        else:\n",
    "            temp = [[i, log[i]] for i in token_list]\n",
    "        temp.sort(key=lambda var: var[1], reverse=True)\n",
    "        rank.append([var[0] for var in temp].index(token))\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "def return_rank_norm_layer(hd, word_embedding_, token, metric='dot', token_list=None, layer_norm=None):\n",
    "    \"\"\"\n",
    "    Returns the rank of a given token based on similarity between hidden states and word embeddings.\n",
    "\n",
    "    Args:\n",
    "        hd (torch.Tensor): Hidden states of shape (batch_size, hidden_dim).\n",
    "        word_embedding_ (torch.Tensor): Word embedding matrix of shape (vocab_size, hidden_dim).\n",
    "        token (int): Target token index whose rank is to be found.\n",
    "        metric (str): Similarity metric ('dot' or 'cos'). Defaults to 'dot'.\n",
    "        token_list (list, optional): List of token indices to consider. Defaults to None.\n",
    "        layer_norm (LayerNorm, optional): LayerNorm instance for final layer normalization. \n",
    "                                          If None, no normalization is applied. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        list: Ranks of the target token for each instance in the batch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Apply final layer normalization if provided\n",
    "    if layer_norm is not None:\n",
    "        hd = layer_norm(hd)\n",
    "\n",
    "    # Normalize word embeddings if metric is cosine similarity\n",
    "    if metric == 'cos':\n",
    "        word_embedding = F.normalize(word_embedding_, p=2, dim=1)\n",
    "    elif metric == 'dot':\n",
    "        word_embedding = word_embedding_\n",
    "    else:\n",
    "        raise ValueError(\"Invalid metric. Use 'dot' or 'cos'.\")\n",
    "\n",
    "    # Compute logits by projecting hidden states onto the embedding matrix\n",
    "    logits_ = torch.matmul(hd, word_embedding.T)\n",
    "\n",
    "    # Calculate rank of the target token\n",
    "    rank = []\n",
    "    for log in logits_:\n",
    "        log_np = log.cpu().numpy()\n",
    "        candidates = [[i, log_np[i]] for i in (token_list if token_list is not None else range(len(log_np)))]\n",
    "        candidates.sort(key=lambda var: var[1], reverse=True)\n",
    "        rank.append([var[0] for var in candidates].index(token))\n",
    "\n",
    "    return rank\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#dataset = \"/home/s220331/GROK/Thesis/data/composition_SMALL.200.20.12.6\"\n",
    "dataset=\"/home/s220331/GROK/Thesis/data/composition.2000.200.12.6\"\n",
    "model_path = \"/dtu-compute/s220331/composition/outputs_BIG_new/checkpoint-1500000/\"\n",
    "#\"/dtu-compute/s220331/composition/outputs_BIG_new/checkpoint-1500000/\" \n",
    "\n",
    "target_layer = 8\n",
    "\n",
    "\n",
    "# model set up\n",
    "#device = torch.device('cuda:5')\n",
    "device = setup_device()\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "word_embedding = model.lm_head.weight.data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test case to remove\n",
    "#dataset = \"/home/s220331/GROK/Thesis/data/composition_SMALL.200.20.12.6\"\n",
    "#model_path = \"/scratch/davide/model_paper/outputs_SMALL/checkpoint-4000/\" \n",
    "\n",
    "dataset=\"/home/s220331/GROK/Thesis/data/composition.2000.200.12.6\"  \n",
    "#model_path = \"/dtu-compute/s220331/composition/outputs_BIG_extream_training/checkpoint-1200000/\"\n",
    "#model_path = \"/dtu-compute/s220331/composition/outputs_BIG_extream_training/checkpoint-2250000/\" # \"/dtu-compute/s220331/composition/outputs_BIG_new/checkpoint-1500000/\"\n",
    "#model_path = \"/dtu-compute/s220331/composition/outputs_BIG_extream_training/checkpoint-50000/\" # \"/dtu-compute/s220331/composition/outputs_BIG_new/checkpoint-1500000/\"\n",
    "#model_path = \"/dtu-compute/s220331/composition/outputs_BIG_new/checkpoint-700000/\"\n",
    "#model_path = \"/dtu-compute/s220331/composition/outputs_BIG_extream_training/checkpoint-700000/\"\n",
    "model_path = \"/dtu-compute/s220331/composition/outputs_BIG_extream_training/checkpoint-400000/\"\n",
    "#model_path = \"/dtu-compute/s220331/composition/outputs_BIG_extream_training/checkpoint-800000/\"\n",
    "#model_path = \"/scratch/davide/model_paper/outputs_SMALL/checkpoint-4000/\" \n",
    "target_layer = 8\n",
    "# model set up\n",
    "#device = torch.device('cuda:5')\n",
    "device = setup_device()\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path).to(device)\n",
    "word_embedding = model.lm_head.weight.data\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "tokenizer.padding_side = \"left\" \n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "model.config.pad_token_id = model.config.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 518800/518800 [00:00<00:00, 1383150.09it/s]\n",
      "100%|██████████| 518800/518800 [00:01<00:00, 400075.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# id_atomic, # ood_atomic: 38000 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# set data\n",
    "\n",
    "all_atomic = set()     # (h,r,t)\n",
    "atomic_dict = dict()   # (h,r) -> t\n",
    "with open(dataset+\"/train.json\") as f:  # from the correct data or data_MIO !!!\n",
    "    train_items = json.load(f)\n",
    "for item in tqdm(train_items):\n",
    "    temp = item['target_text'].strip(\"><\").split(\"><\")\n",
    "    if len(temp) != 4:\n",
    "        continue\n",
    "    h,r,t = temp[:3]\n",
    "    atomic_dict[(h,r)] = t\n",
    "    all_atomic.add((h,r,t))\n",
    "\n",
    "id_atomic = set()\n",
    "for item in tqdm(train_items):\n",
    "    temp = item['target_text'].strip(\"><\").split(\"><\")\n",
    "    if len(temp) == 4:\n",
    "        continue\n",
    "    h, r1, r2, t = temp[:4]\n",
    "    b = atomic_dict[(h, r1)]\n",
    "    assert atomic_dict[(b, r2)] == t\n",
    "    id_atomic.add((h,r1,b))\n",
    "    id_atomic.add((b,r2,t))\n",
    "\n",
    "ood_atomic = all_atomic - id_atomic\n",
    "print(\"# id_atomic, # ood_atomic:\", len(id_atomic), len(ood_atomic))\n",
    "\n",
    "# smart way to save all the train\n",
    "with open(dataset+\"/test.json\") as f:\n",
    "    pred_data = json.load(f)\n",
    "d = dict()\n",
    "for item in pred_data:\n",
    "    t = item['type']\n",
    "    if t not in d:\n",
    "        d[t] = []\n",
    "    d[t].append(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To chose the \"B\" that compare 2 times both in ID and OOD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a= chose_B(199)\n",
    "#a[1][0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_change_OOD =\"<e_147><r_4><r_1>\" #\"<e_181><r_6><r_5>\"# \"<e_147><r_4><r_1>\"# \"<e_14><r_12><r_13>\"       # 2 hop ['<e_11><r_1><e_141></a>'] \n",
    "\n",
    "def run_query(query_change_OOD, ood=False):\n",
    "    # inser here the 2-hop query (like ood) from whihc extract the first hop-- \"b\" activations\n",
    "    # usato \"<e_147><r_4><r_1>\"\n",
    "    h_1_ood,n_r_1_ood,r_1_ood = query_change_OOD.strip(\"><\").split(\"><\") #\"e_140\",\"r_19\",\"r_11\"             #query.strip(\"><\").split(\"><\")\n",
    "    b_1_ood = atomic_dict[(h_1_ood ,n_r_1_ood)]\n",
    "    t_1_ood = atomic_dict[(b_1_ood, r_1_ood)]\n",
    "    list_b_t=[]\n",
    "\n",
    "    print(\"b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \",b_1_ood)\n",
    "    print(\"tail:\",t_1_ood)\n",
    "\n",
    "\n",
    "    # Tokenizing the Query:\n",
    "    decoder_temp_1_OOD = tokenizer([query_change_OOD], return_tensors=\"pt\", padding=True)\n",
    "    decoder_input_ids_1_OOD, decoder_attention_mask_1_OOD = decoder_temp_1_OOD[\"input_ids\"], decoder_temp_1_OOD[\"attention_mask\"]\n",
    "    decoder_input_ids_1_OOD, decoder_attention_mask_1_OOD = decoder_input_ids_1_OOD.to(device), decoder_attention_mask_1_OOD.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # here the same model but with different input !! \n",
    "        outputs_1_OOD = model(\n",
    "            input_ids=decoder_input_ids_1_OOD,\n",
    "            attention_mask=decoder_attention_mask_1_OOD,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    # hidden state of the 1_id (which will be inserted in the normal run)\n",
    "    all_hidden_states_1_OOD = outputs_1_OOD['hidden_states']\n",
    "\n",
    "    #################### just to quick check everything is right\n",
    "\n",
    "    res_dict_b_1_OOD = dict()\n",
    "    target_layer_b = 5\n",
    "    target_layer_t = 8\n",
    "    # (\"<\"+ b +\">\")  is just for reconstructing (\"<b>\") from \"b\"\n",
    "    rank_before_1_OOD = return_rank_norm_layer(all_hidden_states_1_OOD[target_layer_b][0, :, :], word_embedding, tokenizer(\"<\"+ b_1_ood +\">\")['input_ids'][0])[-2]\n",
    "    res_dict_b_1_OOD['rank_before, layer 5 b'] = rank_before_1_OOD\n",
    "\n",
    "    # laste layer search for t\n",
    "    rank_before_1_OOD_t = return_rank_norm_layer(all_hidden_states_1_OOD[target_layer_t][0, :, :], word_embedding, tokenizer(\"<\"+ t_1_ood +\">\")['input_ids'][0])[-1]\n",
    "    res_dict_b_1_OOD['rank_before, layer 8 search t'] = rank_before_1_OOD_t\n",
    "    list_b_t.append((rank_before_1_OOD, rank_before_1_OOD_t))\n",
    "    if ood:\n",
    "        print(\"OOD\")\n",
    "        print(\" B 1_OOD\",res_dict_b_1_OOD)\n",
    "        print(\"as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\")\n",
    "    else:\n",
    "        print(\"ID\")\n",
    "        print(\" B 1_OOD\",res_dict_b_1_OOD)\n",
    "        print(\"as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\")\n",
    "\n",
    "    return all_hidden_states_1_OOD , b_1_ood ,t_1_ood ,list_b_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict_OOD_intervention= dict()\n",
    "def replace_hidden_state(hidden_to_insert,b, all_hidden_states_to_change,t):\n",
    "    list_intervention_results = []\n",
    "    target_layer_intervention = 5\n",
    "    target_layer_final = 8\n",
    "    # juest rename for more standardized structure\n",
    "    all_hidden_states_ctft_1 = hidden_to_insert\n",
    "\n",
    "    # we are analysing  with replacing only specific hidden state once a time\n",
    "    for layer_to_intervene in range(target_layer_intervention , target_layer_intervention +1):\n",
    "        print(layer_to_intervene)\n",
    "        hidden_states_1 = all_hidden_states_to_change[layer_to_intervene].clone()    # cloniamo gli hidden del normal run\n",
    "        hidden_states_ctft_1 = all_hidden_states_ctft_1[layer_to_intervene]  # copiamo quelli del run che inseriremo\n",
    "    #r_2  r\n",
    "        #r_2_ood = all_hidden_states_1_OOD_test[layer_to_intervene]\n",
    "\n",
    "        # intervene\n",
    "        hidden_states_1[0, 0, :] = hidden_states_ctft_1[0, 0, :]   #insereiamo gli hidden state che vogliamo\n",
    "        hidden_states_1[0, 1, :] = hidden_states_ctft_1[0, 1, :]\n",
    "        #hidden_states_1[0, 2, :] = hidden_states_ctft_1[0, 2, :]  # prova ad inserire r_2 di ood\n",
    "        #hidden_states_1[0, 2, :]= r_11_id\n",
    "        #hidden_states_1[0, 2, :]= r_2_ood[0, 2, :] # this is a r(second hop) coming from another ood (with not even a;r_1 the same. This prove that r_2 does not \n",
    "        # store calculation in the first 5 layers according to paper results.)\n",
    "        \n",
    "\n",
    "        rank_middle_1 = return_rank_norm_layer(hidden_states_1[0, :, :], word_embedding, tokenizer(\"<\"+b+\">\")['input_ids'][0])[-2]\n",
    "        res_dict_OOD_intervention[\"insertion at layer\"+str(layer_to_intervene), \"B\"] = rank_middle_1\n",
    "        #                \n",
    "        with torch.no_grad():\n",
    "            for i in range(layer_to_intervene, target_layer_final):\n",
    "                f_layer_1 = model.transformer.h[i]  # current layer\n",
    "                print(\"layer aggiornato:\",i)\n",
    "                # attention mechanism \n",
    "                residual_1 = hidden_states_1       #store the hidden state in residual\n",
    "                # Apply layer normalization to hidden_states.\n",
    "                hidden_states_1 = f_layer_1.ln_1(hidden_states_1)\n",
    "                attn_output_1 = f_layer_1.attn(hidden_states_1)[0] \n",
    "                hidden_states_1 = attn_output_1 + residual_1\n",
    "                # mlp  = Feed-Forward Network:\n",
    "                residual_1 = hidden_states_1\n",
    "                hidden_states_1 = f_layer_1.ln_2(hidden_states_1)\n",
    "                feed_forward_hidden_states_1 = f_layer_1.mlp.c_proj(f_layer_1.mlp.act(f_layer_1.mlp.c_fc(hidden_states_1)))\n",
    "                hidden_states_1 = residual_1 + feed_forward_hidden_states_1\n",
    "            # final ln\n",
    "            hidden_states_1 = model.transformer.ln_f(hidden_states_1)\n",
    "        # print(\"--------\")\n",
    "        rank_after_1 = return_rank(hidden_states_1[0, :, :], word_embedding, tokenizer(\"<\"+t+\">\")['input_ids'][0])[-1]\n",
    "        res_dict_OOD_intervention[\"Out-tput layer\"+str(target_layer_final), \"t\"] = rank_after_1\n",
    "        list_intervention_results.append(rank_middle_1)\n",
    "        list_intervention_results.append(rank_after_1)\n",
    "        print(\"intervention results\",res_dict_OOD_intervention)\n",
    "    print( list_intervention_results)\n",
    "    return list_intervention_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Intervention with OOD insertion**\n",
    "\n",
    "What: Insert in the normal run (ID run) the 5 layer hydden layers coming from a OOD run representing the same Bridge entity and r_2 relation.\n",
    "It has be seen that the model still predict B (as expected as also the OOD predict the B) but also the t. In other words, does not seem that the model store and use information about how he gets to the Bridge. Given one Bridge he is able to secondo hop if the second hop is in ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------Target : <e_1>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_1678><r_82><e_1></a>'] \n",
      "\n",
      "OOD ['<e_1182><r_46><e_1></a>', '<e_1270><r_14><e_1></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_1><r_157><e_1608></a>']\n",
      "2nd hop OOD ['<e_1><r_158><e_530></a>', '<e_1><r_121><e_764></a>', '<e_1><r_199><e_1914></a>', '<e_1><r_189><e_736></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_4>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_453><r_87><e_4></a>'] \n",
      "\n",
      "OOD ['<e_1905><r_115><e_4></a>', '<e_1928><r_174><e_4></a>', '<e_1255><r_153><e_4></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_4><r_11><e_363></a>', '<e_4><r_173><e_1710></a>']\n",
      "2nd hop OOD ['<e_4><r_23><e_440></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_17>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_437><r_177><e_17></a>'] \n",
      "\n",
      "OOD ['<e_1743><r_113><e_17></a>', '<e_641><r_129><e_17></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_17><r_193><e_1489></a>']\n",
      "2nd hop OOD ['<e_17><r_93><e_1659></a>'] \n",
      " \n",
      " \n",
      "\n",
      "-------Target : <e_18>      ----------------------------------------------------------\n",
      "\n",
      "ID ['<e_1826><r_67><e_18></a>', '<e_467><r_133><e_18></a>'] \n",
      "\n",
      "OOD ['<e_1380><r_34><e_18></a>', '<e_749><r_76><e_18></a>'] \n",
      " \n",
      "\n",
      "2- hop  ID ['<e_18><r_158><e_674></a>', '<e_18><r_166><e_1282></a>', '<e_18><r_165><e_883></a>', '<e_18><r_162><e_335></a>']\n",
      "2nd hop OOD ['<e_18><r_131><e_481></a>', '<e_18><r_119><e_1389></a>'] \n",
      " \n",
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "atomic_to_chose = chose_B(200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID_ID--------------------------------------------------------------------------- <e_1678><r_82><r_157>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1182><r_46><r_158>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_1\n",
      "tail: e_1608\n",
      "Alpha scaling found. Using alpha = 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_1\n",
      "tail: e_530\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 725}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_1\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 117}\n",
      "[0, 117]\n",
      "ID_ID--------------------------------------------------------------------------- <e_453><r_87><r_11>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1905><r_115><r_23>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_4\n",
      "tail: e_363\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_4\n",
      "tail: e_440\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 657}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_4\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_437><r_177><r_193>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1743><r_113><r_93>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_17\n",
      "tail: e_1489\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_17\n",
      "tail: e_1659\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 977}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_17\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 427}\n",
      "[0, 427]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1826><r_67><r_158>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1380><r_34><r_131>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_18\n",
      "tail: e_674\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_18\n",
      "tail: e_481\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 144}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_18\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 51}\n",
      "[0, 51]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1437><r_52><r_4>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1242><r_184><r_81>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_21\n",
      "tail: e_242\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_21\n",
      "tail: e_887\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 1, 'rank_before, layer 8 search t': 749}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_21\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 1, ('Out-tput layer8', 't'): 112}\n",
      "[1, 112]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1140><r_198><r_99>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_767><r_25><r_189>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_23\n",
      "tail: e_697\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_23\n",
      "tail: e_865\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 803}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_23\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1192><r_127><r_161>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_278><r_109><r_163>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_25\n",
      "tail: e_323\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_25\n",
      "tail: e_958\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1713}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_25\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 670}\n",
      "[0, 670]\n",
      "ID_ID--------------------------------------------------------------------------- <e_635><r_119><r_139>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_516><r_26><r_131>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_27\n",
      "tail: e_1988\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_27\n",
      "tail: e_1875\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 1, 'rank_before, layer 8 search t': 500}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_27\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 1, ('Out-tput layer8', 't'): 55}\n",
      "[1, 55]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1633><r_48><r_149>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1333><r_16><r_181>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_29\n",
      "tail: e_720\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_29\n",
      "tail: e_1934\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1218}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_29\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 360}\n",
      "[0, 360]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1473><r_153><r_17>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1537><r_143><r_112>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_31\n",
      "tail: e_75\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_31\n",
      "tail: e_378\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1099}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_31\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 233}\n",
      "[0, 233]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1827><r_117><r_71>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1839><r_94><r_84>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_37\n",
      "tail: e_813\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_37\n",
      "tail: e_531\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1333}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_37\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 1}\n",
      "[0, 1]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1149><r_50><r_8>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1448><r_80><r_100>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_40\n",
      "tail: e_1732\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_40\n",
      "tail: e_1041\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 224}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_40\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 1}\n",
      "[0, 1]\n",
      "ID_ID--------------------------------------------------------------------------- <e_998><r_46><r_41>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1793><r_121><r_145>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_47\n",
      "tail: e_1183\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_47\n",
      "tail: e_896\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 3, 'rank_before, layer 8 search t': 16}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_47\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 3, ('Out-tput layer8', 't'): 291}\n",
      "[3, 291]\n",
      "ID_ID--------------------------------------------------------------------------- <e_820><r_22><r_84>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1392><r_26><r_131>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_49\n",
      "tail: e_67\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_49\n",
      "tail: e_1230\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 755}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_49\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1903><r_148><r_88>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1707><r_16><r_182>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_56\n",
      "tail: e_1033\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_56\n",
      "tail: e_238\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1636}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_56\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 3}\n",
      "[0, 3]\n",
      "ID_ID--------------------------------------------------------------------------- <e_859><r_52><r_45>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1032><r_67><r_3>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_59\n",
      "tail: e_1097\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_59\n",
      "tail: e_1520\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 550}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_59\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 237}\n",
      "[0, 237]\n",
      "ID_ID--------------------------------------------------------------------------- <e_886><r_56><r_159>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_70><r_57><r_32>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_66\n",
      "tail: e_1873\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_66\n",
      "tail: e_898\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 918}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_66\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 473}\n",
      "[0, 473]\n",
      "ID_ID--------------------------------------------------------------------------- <e_49><r_84><r_95>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_417><r_145><r_60>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_67\n",
      "tail: e_421\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_67\n",
      "tail: e_1888\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1695}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_67\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 67}\n",
      "[0, 67]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1405><r_128><r_44>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1342><r_181><r_118>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_68\n",
      "tail: e_1145\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_68\n",
      "tail: e_1272\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1064}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_68\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1821><r_159><r_187>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_351><r_114><r_112>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_70\n",
      "tail: e_298\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_70\n",
      "tail: e_1840\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 679}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_70\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 180}\n",
      "[0, 180]\n",
      "ID_ID--------------------------------------------------------------------------- <e_796><r_43><r_100>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1317><r_182><r_144>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_79\n",
      "tail: e_846\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_79\n",
      "tail: e_1068\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 741}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_79\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1770><r_22><r_0>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1233><r_184><r_32>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_91\n",
      "tail: e_1158\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_91\n",
      "tail: e_108\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 1, 'rank_before, layer 8 search t': 650}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_91\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 1, ('Out-tput layer8', 't'): 28}\n",
      "[1, 28]\n",
      "ID_ID--------------------------------------------------------------------------- <e_396><r_29><r_168>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1265><r_126><r_150>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_92\n",
      "tail: e_1435\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_92\n",
      "tail: e_565\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1382}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_92\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 300}\n",
      "[0, 300]\n",
      "ID_ID--------------------------------------------------------------------------- <e_493><r_81><r_76>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1696><r_198><r_66>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_95\n",
      "tail: e_1230\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_95\n",
      "tail: e_1231\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 710}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_95\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 44}\n",
      "[0, 44]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1646><r_117><r_53>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_533><r_119><r_172>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_96\n",
      "tail: e_1283\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_96\n",
      "tail: e_230\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 190}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_96\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_896><r_16><r_42>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1163><r_31><r_37>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_105\n",
      "tail: e_797\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_105\n",
      "tail: e_1752\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 1, 'rank_before, layer 8 search t': 1717}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_105\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 1, ('Out-tput layer8', 't'): 236}\n",
      "[1, 236]\n",
      "ID_ID--------------------------------------------------------------------------- <e_356><r_3><r_103>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_287><r_15><r_8>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_116\n",
      "tail: e_1077\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_116\n",
      "tail: e_814\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 4, 'rank_before, layer 8 search t': 237}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_116\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 4, ('Out-tput layer8', 't'): 110}\n",
      "[4, 110]\n",
      "ID_ID--------------------------------------------------------------------------- <e_745><r_117><r_36>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1994><r_34><r_50>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_120\n",
      "tail: e_1456\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_120\n",
      "tail: e_1644\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1985}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_120\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 20}\n",
      "[0, 20]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1795><r_9><r_141>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1524><r_50><r_82>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_121\n",
      "tail: e_1066\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_121\n",
      "tail: e_1759\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 1, 'rank_before, layer 8 search t': 406}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_121\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 1, ('Out-tput layer8', 't'): 136}\n",
      "[1, 136]\n",
      "ID_ID--------------------------------------------------------------------------- <e_860><r_71><r_10>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_718><r_30><r_168>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_126\n",
      "tail: e_1953\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_126\n",
      "tail: e_536\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1984}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_126\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 18}\n",
      "[0, 18]\n",
      "ID_ID--------------------------------------------------------------------------- <e_303><r_89><r_61>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1680><r_88><r_44>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_127\n",
      "tail: e_596\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_127\n",
      "tail: e_74\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 217}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_127\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 50}\n",
      "[0, 50]\n",
      "ID_ID--------------------------------------------------------------------------- <e_754><r_169><r_127>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_334><r_170><r_42>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_128\n",
      "tail: e_367\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_128\n",
      "tail: e_1240\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 436}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_128\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 35}\n",
      "[0, 35]\n",
      "ID_ID--------------------------------------------------------------------------- <e_686><r_171><r_103>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1183><r_116><r_148>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_131\n",
      "tail: e_1861\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_131\n",
      "tail: e_43\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 1, 'rank_before, layer 8 search t': 686}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_131\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 1, ('Out-tput layer8', 't'): 1272}\n",
      "[1, 1272]\n",
      "ID_ID--------------------------------------------------------------------------- <e_69><r_137><r_33>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1540><r_8><r_125>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_132\n",
      "tail: e_1133\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_132\n",
      "tail: e_1795\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 846}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_132\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 41}\n",
      "[0, 41]\n",
      "ID_ID--------------------------------------------------------------------------- <e_327><r_50><r_17>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1774><r_120><r_176>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_133\n",
      "tail: e_72\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_133\n",
      "tail: e_898\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 695}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_133\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 181}\n",
      "[0, 181]\n",
      "ID_ID--------------------------------------------------------------------------- <e_484><r_135><r_197>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_951><r_5><r_140>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_136\n",
      "tail: e_1331\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_136\n",
      "tail: e_405\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 246}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_136\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 918}\n",
      "[0, 918]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1261><r_129><r_133>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_879><r_101><r_174>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_138\n",
      "tail: e_355\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_138\n",
      "tail: e_395\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1083}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_138\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 583}\n",
      "[0, 583]\n",
      "ID_ID--------------------------------------------------------------------------- <e_609><r_178><r_73>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1256><r_122><r_65>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_140\n",
      "tail: e_599\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_140\n",
      "tail: e_1491\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 377}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_140\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 369}\n",
      "[0, 369]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1741><r_20><r_140>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1539><r_147><r_89>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_143\n",
      "tail: e_1981\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_143\n",
      "tail: e_976\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 501}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_143\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 27}\n",
      "[0, 27]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1945><r_180><r_99>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1309><r_22><r_75>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_146\n",
      "tail: e_505\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_146\n",
      "tail: e_1707\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 542}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_146\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 439}\n",
      "[0, 439]\n",
      "ID_ID--------------------------------------------------------------------------- <e_271><r_6><r_92>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_209><r_37><r_184>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_148\n",
      "tail: e_1721\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_148\n",
      "tail: e_1013\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 1, 'rank_before, layer 8 search t': 126}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_148\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 1, ('Out-tput layer8', 't'): 6}\n",
      "[1, 6]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1092><r_155><r_13>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1866><r_99><r_197>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_153\n",
      "tail: e_52\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_153\n",
      "tail: e_1222\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 6, 'rank_before, layer 8 search t': 623}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_153\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 6, ('Out-tput layer8', 't'): 197}\n",
      "[6, 197]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1790><r_184><r_66>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1968><r_60><r_169>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_158\n",
      "tail: e_570\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_158\n",
      "tail: e_399\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 304}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_158\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 154}\n",
      "[0, 154]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1665><r_83><r_110>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_271><r_55><r_92>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_166\n",
      "tail: e_245\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_166\n",
      "tail: e_1686\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 132}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_166\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_164><r_193><r_32>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1997><r_170><r_16>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_171\n",
      "tail: e_1808\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_171\n",
      "tail: e_1765\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 1486}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_171\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 3}\n",
      "[0, 3]\n",
      "ID_ID--------------------------------------------------------------------------- <e_920><r_42><r_175>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1716><r_130><r_117>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_173\n",
      "tail: e_1113\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_173\n",
      "tail: e_1866\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 777}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_173\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 1}\n",
      "[0, 1]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1081><r_87><r_149>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_24><r_159><r_146>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_177\n",
      "tail: e_1734\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_177\n",
      "tail: e_142\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 342}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_177\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 9}\n",
      "[0, 9]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1768><r_49><r_89>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1771><r_178><r_18>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_181\n",
      "tail: e_789\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_181\n",
      "tail: e_509\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 686}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_181\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1127><r_76><r_192>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1092><r_76><r_115>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_182\n",
      "tail: e_123\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_182\n",
      "tail: e_1102\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 52190}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_182\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n",
      "ID_ID--------------------------------------------------------------------------- <e_1172><r_161><r_101>\n",
      "OOD_OOD------------------------------------------------------------------------------------ <e_1689><r_103><r_124>\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_183\n",
      "tail: e_803\n",
      "Alpha scaling found. Using alpha = 1\n",
      "ID\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 0}\n",
      "as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\n",
      "b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa  e_183\n",
      "tail: e_1544\n",
      "Alpha scaling found. Using alpha = 1\n",
      "OOD\n",
      " B 1_OOD {'rank_before, layer 5 b': 0, 'rank_before, layer 8 search t': 580}\n",
      "as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\n",
      "-------------------------------------------------------------------------------------------------------------------b e_183\n",
      "5\n",
      "layer aggiornato: 5\n",
      "layer aggiornato: 6\n",
      "layer aggiornato: 7\n",
      "intervention results {('insertion at layer5', 'B'): 0, ('Out-tput layer8', 't'): 0}\n",
      "[0, 0]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# take 1 from 1_id and 1 from inferred to construct the query\n",
    "ID_ID_list = []\n",
    "OOD_OOD_list = []\n",
    "OOD_intervention_in_ID= []\n",
    "for i in range(0,50):\n",
    "    _1_id = atomic_to_chose[0][i]  # list with all the\n",
    "    _1_ood = atomic_to_chose[1][i]\n",
    "    _1_id_inferred = atomic_to_chose[2][i]\n",
    "    _1_ood_inferred = atomic_to_chose[3][i]\n",
    "\n",
    "    text_1 = _1_id[0]\n",
    "    match = re.match(r'^(<e_\\d+><r_\\d+>)', text_1)\n",
    "    if match:\n",
    "        result_1_id = match.group(1)\n",
    "    text_2= _1_id_inferred[0]\n",
    "    match = re.match(r'^<e_\\d+>(<r_\\d+>)', text_2)\n",
    "    if match:\n",
    "        result2_id = match.group(1)\n",
    "\n",
    "    text_3 = _1_ood[0]\n",
    "    match = re.match(r'^(<e_\\d+><r_\\d+>)', text_3)\n",
    "    if match:\n",
    "        result_1_ood = match.group(1)\n",
    "    text_4= _1_ood_inferred[0]\n",
    "    match = re.match(r'^<e_\\d+>(<r_\\d+>)', text_4)\n",
    "    if match:\n",
    "        result2_ood = match.group(1)\n",
    "\n",
    "    id_id = result_1_id + result2_id\n",
    "    ood_ood = result_1_ood + result2_ood\n",
    "    print(\"ID_ID---------------------------------------------------------------------------\", id_id)   \n",
    "    print(\"OOD_OOD------------------------------------------------------------------------------------\", ood_ood)\n",
    "    normal_run_ID_ID =run_query(id_id, ood=False)\n",
    "    B_OOD_insert_activations =run_query(ood_ood, ood=True) \n",
    "    print(\"-------------------------------------------------------------------------------------------------------------------b\",B_OOD_insert_activations[1])\n",
    "    \n",
    "    intervention=  replace_hidden_state(B_OOD_insert_activations[0],B_OOD_insert_activations[1],normal_run_ID_ID[0], normal_run_ID_ID[2])  # hidden to insert, b, (all hidden states) normal run, t\n",
    "    \n",
    "    ID_ID_list.append(normal_run_ID_ID[3])\n",
    "    OOD_OOD_list.append(B_OOD_insert_activations[3])\n",
    "    OOD_intervention_in_ID.append(intervention)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(atomic_to_chose[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(OOD_OOD_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 117],\n",
       " [0, 0],\n",
       " [0, 427],\n",
       " [0, 51],\n",
       " [1, 112],\n",
       " [0, 0],\n",
       " [0, 670],\n",
       " [1, 55],\n",
       " [0, 360],\n",
       " [0, 233],\n",
       " [0, 1],\n",
       " [0, 1],\n",
       " [3, 291],\n",
       " [0, 0],\n",
       " [0, 3],\n",
       " [0, 237],\n",
       " [0, 473],\n",
       " [0, 67],\n",
       " [0, 0],\n",
       " [0, 180],\n",
       " [0, 0],\n",
       " [1, 28],\n",
       " [0, 300],\n",
       " [0, 44],\n",
       " [0, 0],\n",
       " [1, 236],\n",
       " [4, 110],\n",
       " [0, 20],\n",
       " [1, 136],\n",
       " [0, 18],\n",
       " [0, 50],\n",
       " [0, 35],\n",
       " [1, 1272],\n",
       " [0, 41],\n",
       " [0, 181],\n",
       " [0, 918],\n",
       " [0, 583],\n",
       " [0, 369],\n",
       " [0, 27],\n",
       " [0, 439],\n",
       " [1, 6],\n",
       " [6, 197],\n",
       " [0, 154],\n",
       " [0, 0],\n",
       " [0, 3],\n",
       " [0, 1],\n",
       " [0, 9],\n",
       " [0, 0],\n",
       " [0, 0],\n",
       " [0, 0]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OOD_intervention_in_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)],\n",
       " [(0, 0)]]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ID_ID_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with both nonzero values:\n",
      "   first  second\n",
      "0      0       0\n",
      "1      0       0\n",
      "2      0       0\n",
      "3      0       0\n",
      "4      0       0\n",
      "5      0       0\n",
      "6      0       0\n",
      "7      0       0\n",
      "8      0       0\n",
      "9      0       0\n",
      "\n",
      "DataFrame with first nonzero and second zero:\n",
      "Empty DataFrame\n",
      "Columns: [first, second]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Your data: list of lists with two values each.\n",
    "data1 = OOD_intervention_in_ID\n",
    "\n",
    "# Create a DataFrame with appropriate column names\n",
    "df1 = pd.DataFrame(data1, columns=[\"first\", \"second\"])\n",
    "\n",
    "# DataFrame 1: Rows where both 'first' and 'second' are nonzero\n",
    "df_both_nonzero1 = df1[(df1[\"first\"] == 0) & (df1[\"second\"] == 0)].reset_index(drop=True)\n",
    "\n",
    "# DataFrame 2: Rows where 'first' is nonzero and 'second' is zero\n",
    "df_first_nonzero_second_zero1 = df1[(df1[\"first\"] != 0) & (df1[\"second\"] == 0)].reset_index(drop=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"DataFrame with both nonzero values:\")\n",
    "print(df_both_nonzero1)\n",
    "\n",
    "print(\"\\nDataFrame with first nonzero and second zero:\")\n",
    "print(df_first_nonzero_second_zero1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[73], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with both nonzero values:\n",
      "    first  second\n",
      "0       0       0\n",
      "1       0       0\n",
      "2       0       0\n",
      "3       0       0\n",
      "4       0       0\n",
      "5       0       0\n",
      "6       0       0\n",
      "7       0       0\n",
      "8       0       0\n",
      "9       0       0\n",
      "10      0       0\n",
      "11      0       0\n",
      "12      0       0\n",
      "13      0       0\n",
      "14      0       0\n",
      "\n",
      "DataFrame with first nonzero and second zero:\n",
      "   first  second\n",
      "0      1       0\n",
      "1      4       0\n",
      "2      1       0\n",
      "3     27       0\n",
      "4      1       0\n",
      "5     17       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "#case_2MIL= OOD_intervention_in_ID.copy()\n",
    "# Your data: list of lists with two values each.\n",
    "data = case_2MIL\n",
    "\n",
    "# Create a DataFrame with appropriate column names\n",
    "df = pd.DataFrame(data, columns=[\"first\", \"second\"])\n",
    "\n",
    "# DataFrame 1: Rows where both 'first' and 'second' are nonzero\n",
    "df_both_nonzero = df[(df[\"first\"] == 0) & (df[\"second\"] == 0)].reset_index(drop=True)\n",
    "\n",
    "# DataFrame 2: Rows where 'first' is nonzero and 'second' is zero\n",
    "df_first_nonzero_second_zero = df[(df[\"first\"] != 0) & (df[\"second\"] == 0)].reset_index(drop=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"DataFrame with both nonzero values:\")\n",
    "print(df_both_nonzero)\n",
    "\n",
    "print(\"\\nDataFrame with first nonzero and second zero:\")\n",
    "print(df_first_nonzero_second_zero)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stop' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstop\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stop' is not defined"
     ]
    }
   ],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame with both nonzero values:\n",
      "   first  second\n",
      "0      0       0\n",
      "1      0       0\n",
      "2      0       0\n",
      "3      0       0\n",
      "4      0       0\n",
      "5      0       0\n",
      "\n",
      "DataFrame with first nonzero and second zero:\n",
      "   first  second\n",
      "0      1       0\n",
      "1     15       0\n",
      "2      1       0\n",
      "3      1       0\n",
      "4     16       0\n",
      "5     24       0\n",
      "6      2       0\n",
      "7      1       0\n",
      "8      1       0\n",
      "9     18       0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Your data: list of lists with two values each.\n",
    "data1 = OOD_intervention_in_ID\n",
    "\n",
    "# Create a DataFrame with appropriate column names\n",
    "df1 = pd.DataFrame(data1, columns=[\"first\", \"second\"])\n",
    "\n",
    "# DataFrame 1: Rows where both 'first' and 'second' are nonzero\n",
    "df_both_nonzero1 = df1[(df1[\"first\"] == 0) & (df1[\"second\"] == 0)].reset_index(drop=True)\n",
    "\n",
    "# DataFrame 2: Rows where 'first' is nonzero and 'second' is zero\n",
    "df_first_nonzero_second_zero1 = df1[(df1[\"first\"] != 0) & (df1[\"second\"] == 0)].reset_index(drop=True)\n",
    "\n",
    "# Display the results\n",
    "print(\"DataFrame with both nonzero values:\")\n",
    "print(df_both_nonzero1)\n",
    "\n",
    "print(\"\\nDataFrame with first nonzero and second zero:\")\n",
    "print(df_first_nonzero_second_zero1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PURE B fro layer 0 to lyer 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "res_dict_OOD_intervention_PURE__B= dict()\n",
    "def replace_hidden_state_B(hidden_to_insert,b, all_hidden_states_to_change,t):\n",
    "    list_intervention_results = []\n",
    "    target_layer_intervention = 4\n",
    "    target_layer_final = 8\n",
    "    target_B=0\n",
    "    # juest rename for more standardized structure\n",
    "    all_hidden_states_ctft_1 = hidden_to_insert\n",
    "\n",
    "    # we are analysing  with replacing only specific hidden state once a time\n",
    "    for layer_to_intervene in range(target_layer_intervention , target_layer_intervention +1):\n",
    "        print(layer_to_intervene)\n",
    "        hidden_states_1 = all_hidden_states_to_change[layer_to_intervene].clone()    # cloniamo gli hidden del normal run\n",
    "        hidden_states_ctft_1 = all_hidden_states_ctft_1[target_B]  # copiamo quelli del run che inseriremo\n",
    "    #r_2  r\n",
    "        #r_2_ood = all_hidden_states_1_OOD_test[layer_to_intervene]\n",
    "\n",
    "        # intervene\n",
    "        #hidden_states_1[0, 0, :] = hidden_states_ctft_1[0, 0, :]   #insereiamo gli hidden state che vogliamo\n",
    "        hidden_states_1[0, 1, :] = hidden_states_ctft_1[0, 0, :]\n",
    "        hidden_states_1[0, 2, :] = hidden_states_ctft_1[0, 1, :]  # prova ad inserire r_2 di ood\n",
    "        #hidden_states_1[0, 2, :]= r_11_id\n",
    "        #hidden_states_1[0, 2, :]= r_2_ood[0, 2, :] # this is a r(second hop) coming from another ood (with not even a;r_1 the same. This prove that r_2 does not \n",
    "        # store calculation in the first 5 layers according to paper results.)\n",
    "        \n",
    "\n",
    "        rank_middle_1 = return_rank_norm_layer(hidden_states_1[0, :, :], word_embedding, tokenizer(\"<\"+b+\">\")['input_ids'][0])[-2]\n",
    "        res_dict_OOD_intervention[\"insertion at layer\"+str(layer_to_intervene), \"B\"] = rank_middle_1\n",
    "        #                \n",
    "        with torch.no_grad():\n",
    "            for i in range(layer_to_intervene, target_layer_final):\n",
    "                f_layer_1 = model.transformer.h[i]  # current layer\n",
    "                print(\"layer aggiornato:\",i)\n",
    "                # attention mechanism \n",
    "                residual_1 = hidden_states_1       #store the hidden state in residual\n",
    "                # Apply layer normalization to hidden_states.\n",
    "                hidden_states_1 = f_layer_1.ln_1(hidden_states_1)\n",
    "                attn_output_1 = f_layer_1.attn(hidden_states_1)[0] \n",
    "                hidden_states_1 = attn_output_1 + residual_1\n",
    "                # mlp  = Feed-Forward Network:\n",
    "                residual_1 = hidden_states_1\n",
    "                hidden_states_1 = f_layer_1.ln_2(hidden_states_1)\n",
    "                feed_forward_hidden_states_1 = f_layer_1.mlp.c_proj(f_layer_1.mlp.act(f_layer_1.mlp.c_fc(hidden_states_1)))\n",
    "                hidden_states_1 = residual_1 + feed_forward_hidden_states_1\n",
    "            # final ln\n",
    "            hidden_states_1 = model.transformer.ln_f(hidden_states_1)\n",
    "        # print(\"--------\")\n",
    "        rank_after_1 = return_rank_norm_layer(hidden_states_1[0, :, :], word_embedding, tokenizer(\"<\"+t+\">\")['input_ids'][0])[-1]\n",
    "        res_dict_OOD_intervention[\"Out-tput layer\"+str(target_layer_final), \"t\"] = rank_after_1\n",
    "        list_intervention_results.append(rank_middle_1)\n",
    "        list_intervention_results.append(rank_after_1)\n",
    "        print(\"intervention results\",res_dict_OOD_intervention)\n",
    "    print( list_intervention_results)\n",
    "    return list_intervention_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_change_OOD =\"<e_147><r_4><r_1>\" #\"<e_181><r_6><r_5>\"# \"<e_147><r_4><r_1>\"# \"<e_14><r_12><r_13>\"       # 2 hop ['<e_11><r_1><e_141></a>'] \n",
    "\n",
    "def run_query_B(query_change_OOD, ood=False):\n",
    "    # inser here the 2-hop query (like ood) from whihc extract the first hop-- \"b\" activations\n",
    "    # usato \"<e_147><r_4><r_1>\"\n",
    "    h_1_ood,n_r_1_ood,r_1_ood = query_change_OOD.strip(\"><\").split(\"><\") #\"e_140\",\"r_19\",\"r_11\"             #query.strip(\"><\").split(\"><\")\n",
    "    b_1_ood = atomic_dict[(h_1_ood ,n_r_1_ood)]\n",
    "    t_1_ood = atomic_dict[(b_1_ood, r_1_ood)]\n",
    "    list_b_t=[]\n",
    "\n",
    "    print(\"b: quaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa \",b_1_ood)\n",
    "    print(\"tail:\",t_1_ood)\n",
    "    \n",
    "    query_B_layer_for_layer_0=   \"<\"+ b_1_ood +\">\"+\"<\"+ r_1_ood +\">\"  # here put B and r-2 to extract it at layer 0 and insert it in the norml run\n",
    "\n",
    "    # Tokenizing the Query:\n",
    "    decoder_temp_1_OOD = tokenizer([query_B_layer_for_layer_0], return_tensors=\"pt\", padding=True)\n",
    "    decoder_input_ids_1_OOD, decoder_attention_mask_1_OOD = decoder_temp_1_OOD[\"input_ids\"], decoder_temp_1_OOD[\"attention_mask\"]\n",
    "    decoder_input_ids_1_OOD, decoder_attention_mask_1_OOD = decoder_input_ids_1_OOD.to(device), decoder_attention_mask_1_OOD.to(device)\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # here the same model but with different input !! \n",
    "        outputs_1_OOD = model(\n",
    "            input_ids=decoder_input_ids_1_OOD,\n",
    "            attention_mask=decoder_attention_mask_1_OOD,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "    # hidden state of the 1_id (which will be inserted in the normal run)\n",
    "    all_hidden_states_1_OOD = outputs_1_OOD['hidden_states']\n",
    "\n",
    "    #################### just to quick check everything is right\n",
    "\n",
    "    res_dict_b_1_OOD = dict()\n",
    "    target_layer_b = 4\n",
    "    target_layer_t = 8\n",
    "    # (\"<\"+ b +\">\")  is just for reconstructing (\"<b>\") from \"b\"\n",
    "    rank_before_1_OOD = return_rank_norm_layer(all_hidden_states_1_OOD[target_layer_b][0, :, :], word_embedding, tokenizer(\"<\"+ b_1_ood +\">\")['input_ids'][0])[-2]\n",
    "    res_dict_b_1_OOD['rank_before, layer 5 b'] = rank_before_1_OOD\n",
    "\n",
    "    # laste layer search for t\n",
    "    rank_before_1_OOD_t = return_rank_norm_layer(all_hidden_states_1_OOD[target_layer_t][0, :, :], word_embedding, tokenizer(\"<\"+ t_1_ood +\">\")['input_ids'][0])[-1]\n",
    "    res_dict_b_1_OOD['rank_before, layer 8 search t'] = rank_before_1_OOD_t\n",
    "    list_b_t.append((rank_before_1_OOD, rank_before_1_OOD_t))\n",
    "    if ood:\n",
    "        print(\"OOD\")\n",
    "        print(\" B 1_OOD\",res_dict_b_1_OOD)\n",
    "        print(\"as expected in the case of OOD the B is corrctly found at layer 5 but not T at layer 8\")\n",
    "    else:\n",
    "        print(\"ID\")\n",
    "        print(\" B 1_OOD\",res_dict_b_1_OOD)\n",
    "        print(\"as expected in the case of ID the B is corrctly found at layer 5 and T at layer 8\")\n",
    "\n",
    "    return all_hidden_states_1_OOD , b_1_ood ,t_1_ood ,list_b_t\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "atomic_to_chose = chose_B(199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# take 1 from 1_id and 1 from inferred to construct the query\n",
    "ID_ID_list_P = []\n",
    "OOD_OOD_list_P = []\n",
    "OOD_intervention_in_ID_P= []\n",
    "for i in range(0,7):\n",
    "    _1_id = atomic_to_chose[0][i]  # list with all the\n",
    "    _1_ood = atomic_to_chose[1][i]\n",
    "    _1_id_inferred = atomic_to_chose[2][i]\n",
    "    _1_ood_inferred = atomic_to_chose[3][i]\n",
    "\n",
    "    text_1 = _1_id[0]\n",
    "    match = re.match(r'^(<e_\\d+><r_\\d+>)', text_1)\n",
    "    if match:\n",
    "        result_1_id = match.group(1)\n",
    "\n",
    "    text_1_1 = _1_id[1]\n",
    "    match_1 = re.match(r'^(<e_\\d+><r_\\d+>)', text_1_1)\n",
    "    if match_1:\n",
    "        result_1_id_1 = match_1.group(1)\n",
    "\n",
    "    \n",
    "    text_2= _1_id_inferred[0]\n",
    "    match = re.match(r'^<e_\\d+>(<r_\\d+>)', text_2)\n",
    "    if match:\n",
    "        result2_id = match.group(1)\n",
    "\n",
    "    text_3 = _1_ood[0]\n",
    "    match = re.match(r'^(<e_\\d+><r_\\d+>)', text_3)\n",
    "    if match:\n",
    "        result_1_ood = match.group(1)\n",
    "    text_4= _1_ood_inferred[0]\n",
    "    match = re.match(r'^<e_\\d+>(<r_\\d+>)', text_4)\n",
    "    if match:\n",
    "        result2_ood = match.group(1)\n",
    "\n",
    "    id_id = result_1_id + result2_id\n",
    "\n",
    "    id_id_1 = result_1_id_1 + result2_id\n",
    "    ood_ood = result_1_ood + result2_ood\n",
    "    print(\"ID_ID---------------------------------------------------------------------------\", id_id)   \n",
    "    print(\"OOD_OOD------------------------------------------------------------------------------------\", ood_ood)\n",
    "    normal_run_ID_ID =run_query(id_id, ood=False)\n",
    "    #B_OOD_insert_activations =run_query(ood_ood, ood=True) \n",
    "    normal_2 =run_query_B(id_id , ood=True)  # in this run the insertion is (h,r_1,b) instead of the normla (h,r_1;r_2), to alloe the b and r_1 extraction at layer 0\n",
    "    #print(\"-------------------------------------------------------------------------------------------------------------------b\",B_OOD_insert_activations[1])\n",
    "    \n",
    "    intervention=  replace_hidden_state_B(normal_2[0],normal_2[1],normal_run_ID_ID[0], normal_run_ID_ID[2])  # hidden to insert, b, (all hidden states) normal run, t\n",
    "    #intervention=  replace_hidden_state_B(normal_2[0],normal_2[1],normal_run_ID_ID[0], normal_run_ID_ID[2])  # hidden to insert, b, (all hidden states) normal run, t\n",
    "    \n",
    "    ID_ID_list_P.append(normal_run_ID_ID[3])\n",
    "    #OOD_OOD_list_P.append(normal_2[3]) # in this case we do not have the second hop insert, and also the inout is different (b,r)instead od (h,r,r)so is normal that we do not find b in fort layer as is insert directlu in layer 0\n",
    "    OOD_intervention_in_ID_P.append(intervention)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_ID_list_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It could be see that by replacing the \"pure B\"= B from layer 0 to the 4th layer (replacing both B and r_2 from layer 0), the b prediction is correct at layer 4 but the final tail, could be not found as the upper layer never see the atomic facts from the input perspective but always after going through the first 4 layer, leading them to not being able to treat the atomicsfrom layer 0. Similalry, the OOD facts ae never seen from the upper layers , this 1) explains the delayed geenralization in recurrent mode 2) shows that the upper layer never see the OOD atomics at all, as at layer 4 they are so changed to actually see B instead of r_1 . so instead of seeing (B,r_1) they see (B,C) = two consecutive entities , leading the model not beeing able to generlaize ood at all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OOD_intervention_in_ID_P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
