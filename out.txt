/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/launch.py:180: FutureWarning: The module torch.distributed.launch is deprecated
and will be removed in future. Use torchrun.
Note that --use_env is set by default in torchrun.
If your script expects `--local_rank` argument to be set, please
change it to read from `os.environ['LOCAL_RANK']` instead. See 
https://pytorch.org/docs/stable/distributed.html#launch-utility for 
further instructions

  warnings.warn(
WARNING:torch.distributed.run:
*****************************************
Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
*****************************************
/home/s220331/GROK/Thesis/transformers/src/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
/home/s220331/GROK/Thesis/transformers/src/transformers/utils/hub.py:123: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.
0it [00:00, ?it/s]0it [00:00, ?it/s]
numpy version:numpy version:  1.26.41.26.4

torch version:torch version:  1.13.1+cu1161.13.1+cu116

transformers version:transformers version:  4.37.0.dev04.37.0.dev0

local gpu count: 2
***In distributed mode, world_size:2***
provided local_rank is 0. Setting rank and gpu both to be the same.
local gpu count: 2
***In distributed mode, world_size:2***
provided local_rank is 1. Setting rank and gpu both to be the same.
setting device complete. device:setting device complete. device:  cuda:1cuda:0

/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
...initing weights...
...initing weights...
### general model args:
LanguageModelingArgs(adafactor_beta1=None, adafactor_clip_threshold=1.0, adafactor_decay_rate=-0.8, adafactor_eps=(1e-30, 0.001), adafactor_relative_step=True, adafactor_scale_parameter=True, adafactor_warmup_init=True, adam_betas=(0.9, 0.999), adam_epsilon=1e-08, best_model_dir='outputs/best_model', cache_dir='cache_dir/', config={}, cosine_schedule_num_cycles=0.5, custom_layer_parameters=[], custom_parameter_groups=[], dataloader_num_workers=0, do_lower_case=False, dynamic_quantize=False, early_stopping_consider_epochs=False, early_stopping_delta=0, early_stopping_metric='eval_loss', early_stopping_metric_minimize=True, early_stopping_patience=3, encoding=None, eval_batch_size=50, evaluate_during_training=True, evaluate_during_training_silent=True, evaluate_during_training_steps=2000, evaluate_during_training_verbose=False, evaluate_each_epoch=True, fp16=True, gradient_accumulation_steps=1, learning_rate=0.0001, local_rank=-1, logging_steps=50, loss_type=None, loss_args={}, manual_seed=42, max_grad_norm=1.0, max_seq_length=10, model_name='gpt2', model_type='gpt2', multiprocessing_chunksize=-1, n_gpu=2, no_cache=False, no_save=False, not_saved_args=[], num_train_epochs=20, optimizer='AdamW', output_dir='/scratch/davide/model_paper/test_cancel_0_cancel_3', overwrite_output_dir=False, polynomial_decay_schedule_lr_end=1e-07, polynomial_decay_schedule_power=1.0, process_count=30, quantized_model=False, reprocess_input_data=True, save_best_model=False, save_eval_checkpoints=False, save_model_every_epoch=False, save_optimizer_and_scheduler=True, save_steps=5000, scheduler='constant_schedule_with_warmup', silent=False, skip_special_tokens=True, tensorboard_dir=None, thread_count=None, tokenizer_name=None, tokenizer_type=None, train_batch_size=50, train_custom_parameters_only=False, use_cached_eval_features=False, use_early_stopping=False, use_hf_datasets=False, use_multiprocessing=False, use_multiprocessing_for_evaluation=True, wandb_kwargs={}, wandb_project=None, warmup_ratio=0.06, warmup_steps=2000, weight_decay=0.01, model_class='LanguageModelingModel', block_size=10, config_name=None, dataset_class=None, dataset_type='None', discriminator_config={}, discriminator_loss_weight=50.0, generator_config={}, max_steps=150000, min_frequency=2, mlm=False, mlm_probability=0.15, sliding_window=False, special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'], stride=0.8, tie_generator_and_discriminator_embeddings=True, vocab_size=None, clean_text=True, handle_chinese_chars=True, special_tokens_list=[], strip_accents=True)
### ddp args:
{'local_rank': 1, 'rank': -1, 'gpu': None, 'world_size': -1, 'dist_url': 'env://', 'dist_backend': 'nccl'}
lm config:
GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "mask_token_id": 50255,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 8,
  "n_positions": 1024,
  "no_ln": false,
  "no_mlp": false,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "share_mlp": false,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "vocab_size": 50488
}

### general model args:
LanguageModelingArgs(adafactor_beta1=None, adafactor_clip_threshold=1.0, adafactor_decay_rate=-0.8, adafactor_eps=(1e-30, 0.001), adafactor_relative_step=True, adafactor_scale_parameter=True, adafactor_warmup_init=True, adam_betas=(0.9, 0.999), adam_epsilon=1e-08, best_model_dir='outputs/best_model', cache_dir='cache_dir/', config={}, cosine_schedule_num_cycles=0.5, custom_layer_parameters=[], custom_parameter_groups=[], dataloader_num_workers=0, do_lower_case=False, dynamic_quantize=False, early_stopping_consider_epochs=False, early_stopping_delta=0, early_stopping_metric='eval_loss', early_stopping_metric_minimize=True, early_stopping_patience=3, encoding=None, eval_batch_size=50, evaluate_during_training=True, evaluate_during_training_silent=True, evaluate_during_training_steps=2000, evaluate_during_training_verbose=False, evaluate_each_epoch=True, fp16=True, gradient_accumulation_steps=1, learning_rate=0.0001, local_rank=-1, logging_steps=50, loss_type=None, loss_args={}, manual_seed=42, max_grad_norm=1.0, max_seq_length=10, model_name='gpt2', model_type='gpt2', multiprocessing_chunksize=-1, n_gpu=2, no_cache=False, no_save=False, not_saved_args=[], num_train_epochs=20, optimizer='AdamW', output_dir='/scratch/davide/model_paper/test_cancel_0_cancel_3', overwrite_output_dir=False, polynomial_decay_schedule_lr_end=1e-07, polynomial_decay_schedule_power=1.0, process_count=30, quantized_model=False, reprocess_input_data=True, save_best_model=False, save_eval_checkpoints=False, save_model_every_epoch=False, save_optimizer_and_scheduler=True, save_steps=5000, scheduler='constant_schedule_with_warmup', silent=False, skip_special_tokens=True, tensorboard_dir=None, thread_count=None, tokenizer_name=None, tokenizer_type=None, train_batch_size=50, train_custom_parameters_only=False, use_cached_eval_features=False, use_early_stopping=False, use_hf_datasets=False, use_multiprocessing=False, use_multiprocessing_for_evaluation=True, wandb_kwargs={}, wandb_project=None, warmup_ratio=0.06, warmup_steps=2000, weight_decay=0.01, model_class='LanguageModelingModel', block_size=10, config_name=None, dataset_class=None, dataset_type='None', discriminator_config={}, discriminator_loss_weight=50.0, generator_config={}, max_steps=150000, min_frequency=2, mlm=False, mlm_probability=0.15, sliding_window=False, special_tokens=['<s>', '<pad>', '</s>', '<unk>', '<mask>'], stride=0.8, tie_generator_and_discriminator_embeddings=True, vocab_size=None, clean_text=True, handle_chinese_chars=True, special_tokens_list=[], strip_accents=True)
### ddp args:
{'local_rank': 0, 'rank': -1, 'gpu': None, 'world_size': -1, 'dist_url': 'env://', 'dist_backend': 'nccl'}
lm config:
GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "mask_token_id": 50255,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 8,
  "n_positions": 1024,
  "no_ln": false,
  "no_mlp": false,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "share_mlp": false,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.37.0.dev0",
  "use_cache": true,
  "vocab_size": 50488
}

  0%|          | 0/51880 [00:00<?, ?it/s]  0%|          | 114/51880 [00:00<00:45, 1137.93it/s]  1%|          | 386/51880 [00:00<00:24, 2067.11it/s]  1%|▏         | 661/51880 [00:00<00:21, 2377.25it/s]  2%|▏         | 939/51880 [00:00<00:20, 2534.45it/s]  2%|▏         | 1216/51880 [00:00<00:19, 2618.10it/s]  3%|▎         | 1482/51880 [00:00<00:19, 2629.90it/s]  3%|▎         | 1760/51880 [00:00<00:18, 2676.72it/s]  4%|▍         | 2033/51880 [00:00<00:18, 2693.53it/s]  4%|▍         | 2313/51880 [00:00<00:18, 2724.58it/s]  5%|▍         | 2593/51880 [00:01<00:17, 2745.31it/s]  6%|▌         | 2868/51880 [00:01<00:17, 2732.69it/s]  6%|▌         | 3146/51880 [00:01<00:17, 2746.86it/s]  7%|▋         | 3426/51880 [00:01<00:17, 2761.98it/s]  7%|▋         | 3703/51880 [00:01<00:17, 2763.97it/s]  8%|▊         | 3983/51880 [00:01<00:17, 2774.33it/s]  8%|▊         | 4261/51880 [00:01<00:17, 2729.36it/s]  9%|▊         | 4535/51880 [00:01<00:17, 2716.58it/s]  9%|▉         | 4807/51880 [00:01<00:17, 2706.69it/s] 10%|▉         | 5078/51880 [00:01<00:17, 2701.02it/s] 10%|█         | 5349/51880 [00:02<00:17, 2697.56it/s] 11%|█         | 5619/51880 [00:02<00:17, 2664.82it/s] 11%|█▏        | 5888/51880 [00:02<00:17, 2672.18it/s] 12%|█▏        | 6160/51880 [00:02<00:17, 2683.93it/s] 12%|█▏        | 6431/51880 [00:02<00:16, 2690.42it/s] 13%|█▎        | 6703/51880 [00:02<00:16, 2696.94it/s] 13%|█▎        | 6973/51880 [00:02<00:16, 2669.75it/s] 14%|█▍        | 7241/51880 [00:02<00:16, 2662.87it/s] 14%|█▍        | 7511/51880 [00:02<00:16, 2672.89it/s] 15%|█▍        | 7779/51880 [00:02<00:16, 2668.61it/s] 16%|█▌        | 8046/51880 [00:03<00:16, 2666.41it/s] 16%|█▌        | 8313/51880 [00:03<00:16, 2657.28it/s] 17%|█▋        | 8579/51880 [00:03<00:16, 2653.11it/s] 17%|█▋        | 8848/51880 [00:03<00:16, 2663.95it/s] 18%|█▊        | 9115/51880 [00:03<00:16, 2656.94it/s] 18%|█▊        | 9381/51880 [00:03<00:15, 2657.71it/s] 19%|█▊        | 9648/51880 [00:03<00:15, 2659.56it/s] 19%|█▉        | 9914/51880 [00:03<00:15, 2632.58it/s] 20%|█▉        | 10182/51880 [00:03<00:15, 2644.17it/s] 20%|██        | 10447/51880 [00:03<00:15, 2643.79it/s] 21%|██        | 10713/51880 [00:04<00:15, 2647.00it/s] 21%|██        | 10979/51880 [00:04<00:15, 2648.22it/s] 22%|██▏       | 11244/51880 [00:04<00:15, 2623.02it/s] 22%|██▏       | 11511/51880 [00:04<00:15, 2636.82it/s] 23%|██▎       | 11775/51880 [00:04<00:15, 2636.24it/s] 23%|██▎       | 12044/51880 [00:04<00:15, 2650.41it/s] 24%|██▎       | 12310/51880 [00:04<00:14, 2648.50it/s] 24%|██▍       | 12575/51880 [00:04<00:14, 2622.52it/s] 25%|██▍       | 12841/51880 [00:04<00:14, 2632.78it/s] 25%|██▌       | 13109/51880 [00:04<00:14, 2644.63it/s] 26%|██▌       | 13374/51880 [00:05<00:14, 2644.45it/s] 26%|██▋       | 13639/51880 [00:05<00:14, 2642.61it/s] 27%|██▋       | 13904/51880 [00:05<00:14, 2610.33it/s] 27%|██▋       | 14167/51880 [00:05<00:14, 2615.80it/s] 28%|██▊       | 14431/51880 [00:05<00:14, 2622.61it/s] 28%|██▊       | 14696/51880 [00:05<00:14, 2629.57it/s] 29%|██▉       | 14961/51880 [00:05<00:14, 2633.65it/s] 29%|██▉       | 15225/51880 [00:05<00:14, 2611.67it/s] 30%|██▉       | 15487/51880 [00:05<00:13, 2612.45it/s] 30%|███       | 15750/51880 [00:05<00:13, 2615.91it/s] 31%|███       | 16012/51880 [00:06<00:13, 2607.83it/s] 31%|███▏      | 16273/51880 [00:06<00:14, 2524.27it/s] 32%|███▏      | 16526/51880 [00:06<00:14, 2504.10it/s] 32%|███▏      | 16790/51880 [00:06<00:13, 2541.74it/s] 33%|███▎      | 17053/51880 [00:06<00:13, 2567.23it/s] 33%|███▎      | 17317/51880 [00:06<00:13, 2587.40it/s] 34%|███▍      | 17582/51880 [00:06<00:13, 2604.97it/s] 34%|███▍      | 17843/51880 [00:06<00:13, 2593.25it/s] 35%|███▍      | 18103/51880 [00:06<00:17, 1876.86it/s] 35%|███▌      | 18361/51880 [00:07<00:16, 2041.32it/s] 36%|███▌      | 18623/51880 [00:07<00:15, 2184.81it/s] 36%|███▋      | 18879/51880 [00:07<00:14, 2282.86it/s] 37%|███▋      | 19140/51880 [00:07<00:13, 2370.57it/s]WARNING:torch.distributed.elastic.agent.server.api:Received 15 death signal, shutting down workers
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 521202 closing signal SIGTERM
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 521203 closing signal SIGTERM
Traceback (most recent call last):
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 195, in <module>
    main()
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 191, in main
    launch(args)
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/launch.py", line 176, in launch
    run(args)
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 237, in launch_agent
    result = agent.run()
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/elastic/metrics/api.py", line 129, in wrapper
    result = f(*args, **kwargs)
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 709, in run
    result = self._invoke_run(role)
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/elastic/agent/server/api.py", line 850, in _invoke_run
    time.sleep(monitor_interval)
  File "/home/s220331/.conda/envs/my_transformers_env/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/api.py", line 62, in _terminate_process_handler
    raise SignalException(f"Process {os.getpid()} got signal: {sigval}", sigval=sigval)
torch.distributed.elastic.multiprocessing.api.SignalException: Process 521199 got signal: 15
